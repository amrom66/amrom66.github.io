<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="linjinbao66">
  <meta name="description" content="打工笔记">
  <meta name="keywords" content="博客 个人 笔记">
  
  <link rel="prev" href="https://linjinbao.github.io/2020/2020-10-14-k8s%E9%83%A8%E7%BD%B2traefik2.2/" />
  <link rel="next" href="https://linjinbao.github.io/2020/2020-10-15-k8s-%E6%98%A0%E5%B0%84%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/" />
  <link rel="canonical" href="https://linjinbao.github.io/2020/2020-10-15-kubeadm%E9%83%A8%E7%BD%B21.13.1/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           kubeadm部署1.13.1 | 打工笔记
       
  </title>
  <meta name="title" content="kubeadm部署1.13.1 | 打工笔记">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/linjinbao.github.io"
    },
    "articleSection" : "posts",
    "name" : "kubeadm部署1.13.1",
    "headline" : "kubeadm部署1.13.1",
    "description" : "1. 配置阿里云yum源 \/etc\/yum.repos.d\/kubernetes.repo\n[kubernetes]\rname=Kubernetes Repo\rbaseurl=https:\/\/mirrors.aliyun.com\/kubernetes\/yum\/repos\/kubernetes-el7-x86_64\/\rgpgcheck=1\rgpgkey=https:\/\/mirrors.aliyun.com\/kubernetes\/yum\/doc\/rpm-package-key.gpg\renabled=1\r##添加gpg验证\rwget https:\/\/mirrors.aliyun.com\/kubernetes\/yum\/doc\/rpm-package-key.gpg\rrpm --import rpm-package-key.gpg\r2. 安装基本工具 yum install kubectl-1.",
    "inLanguage" : "en-us",
    "author" : "linjinbao66",
    "creator" : "linjinbao66",
    "publisher": "linjinbao66",
    "accountablePerson" : "linjinbao66",
    "copyrightHolder" : "linjinbao66",
    "copyrightYear" : "2020",
    "datePublished": "2020-10-15 00:00:00 \x2b0000 UTC",
    "dateModified" : "2020-10-15 00:00:00 \x2b0000 UTC",
    "url" : "https:\/\/linjinbao.github.io\/2020\/2020-10-15-kubeadm%E9%83%A8%E7%BD%B21.13.1\/",
    "wordCount" : "2246",
    "keywords" : [  "打工笔记"]
}
</script>

</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://linjinbao.github.io">打工笔记</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts" title="">博客</a>
                
                <a class="menu-item" href="/categories/" title="">分类</a>
                
                <a class="menu-item" href="/tags/" title="">标签</a>
                
                <a class="menu-item" href="/2020/about/" title="关于">关于</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>
                <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;
                <a href="https://linjinbao.github.io">打工笔记</a>
            </div>
            <div class="menu-toggle">
                <svg t="1618556759902" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1299" width="16" height="16"><path d="M896 1024h-213.333333a128 128 0 0 1-128-128v-213.333333a128 128 0 0 1 128-128h213.333333a128 128 0 0 1 128 128v213.333333a128 128 0 0 1-128 128z m-213.333333-384a42.666667 42.666667 0 0 0-42.666667 42.666667v213.333333a42.666667 42.666667 0 0 0 42.666667 42.666667h213.333333a42.666667 42.666667 0 0 0 42.666667-42.666667v-213.333333a42.666667 42.666667 0 0 0-42.666667-42.666667z m-341.333334 384H128a128 128 0 0 1-128-128v-213.333333a128 128 0 0 1 128-128h213.333333a128 128 0 0 1 128 128v213.333333a128 128 0 0 1-128 128z m-213.333333-384a42.666667 42.666667 0 0 0-42.666667 42.666667v213.333333a42.666667 42.666667 0 0 0 42.666667 42.666667h213.333333a42.666667 42.666667 0 0 0 42.666667-42.666667v-213.333333a42.666667 42.666667 0 0 0-42.666667-42.666667z m768-170.666667h-213.333333a128 128 0 0 1-128-128V128a128 128 0 0 1 128-128h213.333333a128 128 0 0 1 128 128v213.333333a128 128 0 0 1-128 128z m-213.333333-384a42.666667 42.666667 0 0 0-42.666667 42.666667v213.333333a42.666667 42.666667 0 0 0 42.666667 42.666667h213.333333a42.666667 42.666667 0 0 0 42.666667-42.666667V128a42.666667 42.666667 0 0 0-42.666667-42.666667z m-341.333334 384H128a128 128 0 0 1-128-128V128a128 128 0 0 1 128-128h213.333333a128 128 0 0 1 128 128v213.333333a128 128 0 0 1-128 128zM128 85.333333a42.666667 42.666667 0 0 0-42.666667 42.666667v213.333333a42.666667 42.666667 0 0 0 42.666667 42.666667h213.333333a42.666667 42.666667 0 0 0 42.666667-42.666667V128a42.666667 42.666667 0 0 0-42.666667-42.666667z" p-id="1300"></path></svg>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts" title="">博客</a>
                
                <a class="menu-item" href="/categories/" title="">分类</a>
                
                <a class="menu-item" href="/tags/" title="">标签</a>
                
                <a class="menu-item" href="/2020/about/" title="关于">关于</a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">kubeadm部署1.13.1</h1>
        <div class="post-meta">
                作者： <a itemprop="name" href="https://linjinbao.github.io" rel="author">linjinbao66</a> 时间： 
                <span class="post-time">
                 <time datetime=15109-10-15 itemprop="datePublished">October 15, 2020</time>
                </span>
                分类： 
                
        </div>
    </header>
    <div class="post-content">
        

        

        
        
     
          
          
          

          
          
          

          <h2 id="1-配置阿里云yum源">1. 配置阿里云yum源</h2>
<p>/etc/yum.repos.d/kubernetes.repo</p>
<pre><code class="language-code" data-lang="code">[kubernetes]
name=Kubernetes Repo
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
enabled=1

##添加gpg验证
wget https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
rpm --import rpm-package-key.gpg
</code></pre><h2 id="2-安装基本工具">2. 安装基本工具</h2>
<pre><code class="language-code" data-lang="code">yum install  kubectl-1.13.1 kubelet-1.13.1 kubeadm-1.13.1 -y 
</code></pre><p>备注：如果安装出错，可以分开来执行命令，一个一个安装。注意按照顺序，因为如果先安装kubeadm的话，会默认依赖最新版的Kubelet，导致后续失败。</p>
<h2 id="3-获取镜像">3. 获取镜像</h2>
<p>a.sh</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.13.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.13.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.13.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.13.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.2.24
docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.6

docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.13.1  k8s.gcr.io/kube-apiserver:v1.13.1
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.13.1 k8s.gcr.io/kube-controller-manager:v1.13.1
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.13.1  k8s.gcr.io/kube-scheduler:v1.13.1
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.13.1  k8s.gcr.io/kube-proxy:v1.13.1
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1  k8s.gcr.io/pause:3.1
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.2.24  k8s.gcr.io/etcd:3.2.24
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6

docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.13.1
docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.13.1
docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.13.1
docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.13.1
docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1
docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.2.24
docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.2.6
</code></pre></div><h2 id="4-初始化集群">4. 初始化集群</h2>
<pre><code class="language-code" data-lang="code">kubeadm init --apiserver-advertise-address=10.20.250.21 --kubernetes-version=v1.13.1 --pod-network-cidr=10.244.0.0/16

##结果
Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 10.20.250.21:6443 --token 2yp0f7.rzyji64fqmc09kcv --discovery-token-ca-cert-hash sha256:e3714301edc57a2c0cdbc5aa5c659f50c203aa03c6759756e36ddc96789daff6
</code></pre><h2 id="join命令查询">join命令查询</h2>
<pre><code class="language-code" data-lang="code">kubeadm token create --print-join-command
</code></pre><h2 id="部署calico网络插件">部署calico网络插件</h2>
<p>calico-rbac.yaml</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#75715e"># Calico Version v3.3.2</span>
<span style="color:#75715e"># https://docs.projectcalico.org/v3.3/releases#v3.3.2</span>
<span style="color:#66d9ef">kind</span>: ClusterRole
<span style="color:#66d9ef">apiVersion</span>: rbac.authorization.k8s.io/v1beta1
metadata:
  <span style="color:#66d9ef">name</span>: calico-node
rules:
  - <span style="color:#66d9ef">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
    resources:
      - namespaces
      - serviceaccounts
    verbs:
      - get
      - list
      - watch
  - <span style="color:#66d9ef">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
    resources:
      - pods/status
    verbs:
      - patch
  - <span style="color:#66d9ef">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
    resources:
      - pods
    verbs:
      - get
      - list
      - watch
  - <span style="color:#66d9ef">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
    resources:
      - services
    verbs:
      - get
  - <span style="color:#66d9ef">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
    resources:
      - endpoints
    verbs:
      - get
  - <span style="color:#66d9ef">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
    resources:
      - nodes
    verbs:
      - get
      - list
      - update
      - watch
  - <span style="color:#66d9ef">apiGroups</span>: [<span style="color:#e6db74">&#34;extensions&#34;</span>]
    resources:
      - networkpolicies
    verbs:
      - get
      - list
      - watch
  - <span style="color:#66d9ef">apiGroups</span>: [<span style="color:#e6db74">&#34;networking.k8s.io&#34;</span>]
    resources:
      - networkpolicies
    verbs:
      - watch
      - list
  - <span style="color:#66d9ef">apiGroups</span>: [<span style="color:#e6db74">&#34;crd.projectcalico.org&#34;</span>]
    resources:
      - globalfelixconfigs
      - felixconfigurations
      - bgppeers
      - globalbgpconfigs
      - bgpconfigurations
      - ippools
      - globalnetworkpolicies
      - globalnetworksets
      - networkpolicies
      - clusterinformations
      - hostendpoints
    verbs:
      - create
      - get
      - list
      - update
      - watch

---

<span style="color:#66d9ef">apiVersion</span>: rbac.authorization.k8s.io/v1beta1
<span style="color:#66d9ef">kind</span>: ClusterRoleBinding
metadata:
  <span style="color:#66d9ef">name</span>: calico-node
roleRef:
  <span style="color:#66d9ef">apiGroup</span>: rbac.authorization.k8s.io
  <span style="color:#66d9ef">kind</span>: ClusterRole
  <span style="color:#66d9ef">name</span>: calico-node
subjects:
- <span style="color:#66d9ef">kind</span>: ServiceAccount
  <span style="color:#66d9ef">name</span>: calico-node
  <span style="color:#66d9ef">namespace</span>: kube-system
</code></pre></div><p>calico.yaml</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#75715e"># Calico Version v3.3.2</span>
<span style="color:#75715e"># https://docs.projectcalico.org/v3.3/releases#v3.3.2</span>
<span style="color:#75715e"># This manifest includes the following component versions:</span>
<span style="color:#75715e">#   calico/node:v3.3.2</span>
<span style="color:#75715e">#   calico/cni:v3.3.2</span>

<span style="color:#75715e"># This ConfigMap is used to configure a self-hosted Calico installation.</span>
<span style="color:#66d9ef">kind</span>: ConfigMap
<span style="color:#66d9ef">apiVersion</span>: v1
metadata:
  <span style="color:#66d9ef">name</span>: calico-config
  <span style="color:#66d9ef">namespace</span>: kube-system
data:
  <span style="color:#75715e"># To enable Typha, set this to &#34;calico-typha&#34; *and* set a non-zero value for Typha replicas</span>
  <span style="color:#75715e"># below.  We recommend using Typha if you have more than 50 nodes. Above 100 nodes it is</span>
  <span style="color:#75715e"># essential.</span>
  <span style="color:#66d9ef">typha_service_name</span>: <span style="color:#e6db74">&#34;none&#34;</span>
  <span style="color:#75715e"># Configure the Calico backend to use.</span>
  <span style="color:#66d9ef">calico_backend</span>: <span style="color:#e6db74">&#34;bird&#34;</span>

  <span style="color:#75715e"># Configure the MTU to use</span>
  <span style="color:#66d9ef">veth_mtu</span>: <span style="color:#e6db74">&#34;1440&#34;</span>

  <span style="color:#75715e"># The CNI network configuration to install on each node.  The special</span>
  <span style="color:#75715e"># values in this config will be automatically populated.</span>
  <span style="color:#66d9ef">cni_network_config</span>: |<span style="color:#e6db74">-
</span><span style="color:#e6db74">   </span><span style="color:#e6db74"> </span><span style="color:#e6db74">{</span>
      <span style="color:#66d9ef">&#34;name&#34;: </span><span style="color:#e6db74">&#34;k8s-pod-network&#34;</span>,
      <span style="color:#66d9ef">&#34;cniVersion&#34;: </span><span style="color:#e6db74">&#34;0.3.0&#34;</span>,
      <span style="color:#66d9ef">&#34;plugins&#34;: </span>[
        {
          <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;calico&#34;</span>,
          <span style="color:#66d9ef">&#34;log_level&#34;: </span><span style="color:#e6db74">&#34;info&#34;</span>,
          <span style="color:#66d9ef">&#34;datastore_type&#34;: </span><span style="color:#e6db74">&#34;kubernetes&#34;</span>,
          <span style="color:#66d9ef">&#34;nodename&#34;: </span><span style="color:#e6db74">&#34;__KUBERNETES_NODE_NAME__&#34;</span>,
          <span style="color:#66d9ef">&#34;mtu&#34;: </span>__CNI_MTU__,
          <span style="color:#66d9ef">&#34;ipam&#34;: </span>{
            <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;host-local&#34;</span>,
            <span style="color:#66d9ef">&#34;subnet&#34;: </span><span style="color:#e6db74">&#34;usePodCidr&#34;</span>
          },
          <span style="color:#66d9ef">&#34;policy&#34;: </span>{
              <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;k8s&#34;</span>
          },
          <span style="color:#66d9ef">&#34;kubernetes&#34;: </span>{
              <span style="color:#66d9ef">&#34;kubeconfig&#34;: </span><span style="color:#e6db74">&#34;__KUBECONFIG_FILEPATH__&#34;</span>
          }
        },
        {
          <span style="color:#66d9ef">&#34;type&#34;: </span><span style="color:#e6db74">&#34;portmap&#34;</span>,
          <span style="color:#66d9ef">&#34;snat&#34;: </span><span style="color:#66d9ef">true</span>,
          <span style="color:#66d9ef">&#34;capabilities&#34;: {&#34;portMappings&#34;: </span><span style="color:#66d9ef">true</span>}
        }
      ]
    }

---


<span style="color:#75715e"># This manifest creates a Service, which will be backed by Calico&#39;s Typha daemon.</span>
<span style="color:#75715e"># Typha sits in between Felix and the API server, reducing Calico&#39;s load on the API server.</span>

<span style="color:#66d9ef">apiVersion</span>: v1
<span style="color:#66d9ef">kind</span>: Service
metadata:
  <span style="color:#66d9ef">name</span>: calico-typha
  <span style="color:#66d9ef">namespace</span>: kube-system
  labels:
    <span style="color:#66d9ef">k8s-app</span>: calico-typha
spec:
  ports:
    - <span style="color:#66d9ef">port</span>: <span style="color:#ae81ff">5473</span>
      <span style="color:#66d9ef">protocol</span>: TCP
      <span style="color:#66d9ef">targetPort</span>: calico-typha
      <span style="color:#66d9ef">name</span>: calico-typha
  selector:
    <span style="color:#66d9ef">k8s-app</span>: calico-typha

---

<span style="color:#75715e"># This manifest creates a Deployment of Typha to back the above service.</span>

<span style="color:#66d9ef">apiVersion</span>: apps/v1beta1
<span style="color:#66d9ef">kind</span>: Deployment
metadata:
  <span style="color:#66d9ef">name</span>: calico-typha
  <span style="color:#66d9ef">namespace</span>: kube-system
  labels:
    <span style="color:#66d9ef">k8s-app</span>: calico-typha
spec:
  <span style="color:#75715e"># Number of Typha replicas.  To enable Typha, set this to a non-zero value *and* set the</span>
  <span style="color:#75715e"># typha_service_name variable in the calico-config ConfigMap above.</span>
  <span style="color:#75715e">#</span>
  <span style="color:#75715e"># We recommend using Typha if you have more than 50 nodes.  Above 100 nodes it is essential</span>
  <span style="color:#75715e"># (when using the Kubernetes datastore).  Use one replica for every 100-200 nodes.  In</span>
  <span style="color:#75715e"># production, we recommend running at least 3replicas to reduce the impact of rolling upgrade.</span>
  <span style="color:#66d9ef">replicas</span>: <span style="color:#ae81ff">1</span>
  <span style="color:#66d9ef">revisionHistoryLimit</span>: <span style="color:#ae81ff">2</span>
  template:
    metadata:
      labels:
        <span style="color:#66d9ef">k8s-app</span>: calico-typha
      annotations:
        <span style="color:#75715e"># This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical</span>
        <span style="color:#75715e"># add-on, ensuring it gets priority scheduling and that its resources are reserved</span>
        <span style="color:#75715e"># if it ever gets evicted.</span>
        <span style="color:#66d9ef">scheduler.alpha.kubernetes.io/critical-pod</span>: <span style="color:#e6db74">&#39;&#39;</span>
        <span style="color:#66d9ef">cluster-autoscaler.kubernetes.io/safe-to-evict</span>: <span style="color:#e6db74">&#39;true&#39;</span>
    spec:
      nodeSelector:
        <span style="color:#66d9ef">beta.kubernetes.io/os</span>: linux
      <span style="color:#66d9ef">hostNetwork</span>: <span style="color:#66d9ef">true</span>
      tolerations:
        <span style="color:#75715e"># Mark the pod as a critical add-on for rescheduling.</span>
        - <span style="color:#66d9ef">key</span>: CriticalAddonsOnly
          <span style="color:#66d9ef">operator</span>: Exists
      <span style="color:#75715e"># Since Calico can&#39;t network a pod until Typha is up, we need to run Typha itself</span>
      <span style="color:#75715e"># as a host-networked pod.</span>
      <span style="color:#66d9ef">serviceAccountName</span>: calico-node
      containers:
      - <span style="color:#66d9ef">image</span>: quay.io/calico/typha:v3<span style="color:#ae81ff">.3</span><span style="color:#ae81ff">.2</span>
        <span style="color:#66d9ef">name</span>: calico-typha
        ports:
        - <span style="color:#66d9ef">containerPort</span>: <span style="color:#ae81ff">5473</span>
          <span style="color:#66d9ef">name</span>: calico-typha
          <span style="color:#66d9ef">protocol</span>: TCP
        env:
          <span style="color:#75715e"># Enable &#34;info&#34; logging by default.  Can be set to &#34;debug&#34; to increase verbosity.</span>
          - <span style="color:#66d9ef">name</span>: TYPHA_LOGSEVERITYSCREEN
            <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;info&#34;</span>
          <span style="color:#75715e"># Disable logging to file and syslog since those don&#39;t make sense in Kubernetes.</span>
          - <span style="color:#66d9ef">name</span>: TYPHA_LOGFILEPATH
            <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;none&#34;</span>
          - <span style="color:#66d9ef">name</span>: TYPHA_LOGSEVERITYSYS
            <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;none&#34;</span>
          <span style="color:#75715e"># Monitor the Kubernetes API to find the number of running instances and rebalance</span>
          <span style="color:#75715e"># connections.</span>
          - <span style="color:#66d9ef">name</span>: TYPHA_CONNECTIONREBALANCINGMODE
            <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;kubernetes&#34;</span>
          - <span style="color:#66d9ef">name</span>: TYPHA_DATASTORETYPE
            <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;kubernetes&#34;</span>
          - <span style="color:#66d9ef">name</span>: TYPHA_HEALTHENABLED
            <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;true&#34;</span>
          <span style="color:#75715e"># Uncomment these lines to enable prometheus metrics.  Since Typha is host-networked,</span>
          <span style="color:#75715e"># this opens a port on the host, which may need to be secured.</span>
          <span style="color:#75715e">#- name: TYPHA_PROMETHEUSMETRICSENABLED</span>
          <span style="color:#75715e">#  value: &#34;true&#34;</span>
          <span style="color:#75715e">#- name: TYPHA_PROMETHEUSMETRICSPORT</span>
          <span style="color:#75715e">#  value: &#34;9093&#34;</span>
        livenessProbe:
          exec:
            command:
            - calico-typha
            - check
            - liveness
          <span style="color:#66d9ef">periodSeconds</span>: <span style="color:#ae81ff">30</span>
          <span style="color:#66d9ef">initialDelaySeconds</span>: <span style="color:#ae81ff">30</span>
        readinessProbe:
          exec:
            command:
            - calico-typha
            - check
            - readiness
          <span style="color:#66d9ef">periodSeconds</span>: <span style="color:#ae81ff">10</span>

---

<span style="color:#75715e"># This manifest creates a Pod Disruption Budget for Typha to allow K8s Cluster Autoscaler to evict</span>

<span style="color:#66d9ef">apiVersion</span>: policy/v1beta1
<span style="color:#66d9ef">kind</span>: PodDisruptionBudget
metadata:
  <span style="color:#66d9ef">name</span>: calico-typha
  <span style="color:#66d9ef">namespace</span>: kube-system
  labels:
    <span style="color:#66d9ef">k8s-app</span>: calico-typha
spec:
  <span style="color:#66d9ef">maxUnavailable</span>: <span style="color:#ae81ff">1</span>
  selector:
    matchLabels:
      <span style="color:#66d9ef">k8s-app</span>: calico-typha

---

<span style="color:#75715e"># This manifest installs the calico/node container, as well</span>
<span style="color:#75715e"># as the Calico CNI plugins and network config on</span>
<span style="color:#75715e"># each master and worker node in a Kubernetes cluster.</span>
<span style="color:#66d9ef">kind</span>: DaemonSet
<span style="color:#66d9ef">apiVersion</span>: extensions/v1beta1
metadata:
  <span style="color:#66d9ef">name</span>: calico-node
  <span style="color:#66d9ef">namespace</span>: kube-system
  labels:
    <span style="color:#66d9ef">k8s-app</span>: calico-node
spec:
  selector:
    matchLabels:
      <span style="color:#66d9ef">k8s-app</span>: calico-node
  updateStrategy:
    <span style="color:#66d9ef">type</span>: RollingUpdate
    rollingUpdate:
      <span style="color:#66d9ef">maxUnavailable</span>: <span style="color:#ae81ff">1</span>
  template:
    metadata:
      labels:
        <span style="color:#66d9ef">k8s-app</span>: calico-node
      annotations:
        <span style="color:#75715e"># This, along with the CriticalAddonsOnly toleration below,</span>
        <span style="color:#75715e"># marks the pod as a critical add-on, ensuring it gets</span>
        <span style="color:#75715e"># priority scheduling and that its resources are reserved</span>
        <span style="color:#75715e"># if it ever gets evicted.</span>
        <span style="color:#66d9ef">scheduler.alpha.kubernetes.io/critical-pod</span>: <span style="color:#e6db74">&#39;&#39;</span>
    spec:
      nodeSelector:
        <span style="color:#66d9ef">beta.kubernetes.io/os</span>: linux
      <span style="color:#66d9ef">hostNetwork</span>: <span style="color:#66d9ef">true</span>
      tolerations:
        <span style="color:#75715e"># Make sure calico-node gets scheduled on all nodes.</span>
        - <span style="color:#66d9ef">effect</span>: NoSchedule
          <span style="color:#66d9ef">operator</span>: Exists
        <span style="color:#75715e"># Mark the pod as a critical add-on for rescheduling.</span>
        - <span style="color:#66d9ef">key</span>: CriticalAddonsOnly
          <span style="color:#66d9ef">operator</span>: Exists
        - <span style="color:#66d9ef">effect</span>: NoExecute
          <span style="color:#66d9ef">operator</span>: Exists
      <span style="color:#66d9ef">serviceAccountName</span>: calico-node
      <span style="color:#75715e"># Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &#34;force</span>
      <span style="color:#75715e"># deletion&#34;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.</span>
      <span style="color:#66d9ef">terminationGracePeriodSeconds</span>: <span style="color:#ae81ff">0</span>
      containers:
        <span style="color:#75715e"># Runs calico/node container on each Kubernetes node.  This</span>
        <span style="color:#75715e"># container programs network policy and routes on each</span>
        <span style="color:#75715e"># host.</span>
        - <span style="color:#66d9ef">name</span>: calico-node
          <span style="color:#66d9ef">image</span>: quay.io/calico/node:v3<span style="color:#ae81ff">.3</span><span style="color:#ae81ff">.2</span>
          env:
            <span style="color:#75715e"># Use Kubernetes API as the backing datastore.</span>
            - <span style="color:#66d9ef">name</span>: DATASTORE_TYPE
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;kubernetes&#34;</span>
            <span style="color:#75715e"># Typha support: controlled by the ConfigMap.</span>
            - <span style="color:#66d9ef">name</span>: FELIX_TYPHAK8SSERVICENAME
              valueFrom:
                configMapKeyRef:
                  <span style="color:#66d9ef">name</span>: calico-config
                  <span style="color:#66d9ef">key</span>: typha_service_name
            <span style="color:#75715e"># Wait for the datastore.</span>
            - <span style="color:#66d9ef">name</span>: WAIT_FOR_DATASTORE
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;true&#34;</span>
            <span style="color:#75715e"># Set based on the k8s node name.</span>
            - <span style="color:#66d9ef">name</span>: NODENAME
              valueFrom:
                fieldRef:
                  <span style="color:#66d9ef">fieldPath</span>: spec.nodeName
            <span style="color:#75715e"># Choose the backend to use.</span>
            - <span style="color:#66d9ef">name</span>: CALICO_NETWORKING_BACKEND
              valueFrom:
                configMapKeyRef:
                  <span style="color:#66d9ef">name</span>: calico-config
                  <span style="color:#66d9ef">key</span>: calico_backend
            <span style="color:#75715e"># Cluster type to identify the deployment type</span>
            - <span style="color:#66d9ef">name</span>: CLUSTER_TYPE
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;k8s,bgp&#34;</span>
            <span style="color:#75715e"># Auto-detect the BGP IP address.</span>
            - <span style="color:#66d9ef">name</span>: IP
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;autodetect&#34;</span>
            - <span style="color:#66d9ef">name</span>: IP_AUTODETECTION_METHOD
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;can-reach=10.20.250.21&#34;</span>
            <span style="color:#75715e"># Enable IPIP</span>
            - <span style="color:#66d9ef">name</span>: CALICO_IPV4POOL_IPIP
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;Always&#34;</span>
            <span style="color:#75715e"># Set MTU for tunnel device used if ipip is enabled</span>
            - <span style="color:#66d9ef">name</span>: FELIX_IPINIPMTU
              valueFrom:
                configMapKeyRef:
                  <span style="color:#66d9ef">name</span>: calico-config
                  <span style="color:#66d9ef">key</span>: veth_mtu
            <span style="color:#75715e"># The default IPv4 pool to create on startup if none exists. Pod IPs will be</span>
            <span style="color:#75715e"># chosen from this range. Changing this value after installation will have</span>
            <span style="color:#75715e"># no effect. This should fall within `--cluster-cidr`.</span>
            - <span style="color:#66d9ef">name</span>: CALICO_IPV4POOL_CIDR
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;10.244.0.0/16&#34;</span>
            <span style="color:#75715e"># Disable file logging so `kubectl logs` works.</span>
            - <span style="color:#66d9ef">name</span>: CALICO_DISABLE_FILE_LOGGING
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;true&#34;</span>
            <span style="color:#75715e"># Set Felix endpoint to host default action to ACCEPT.</span>
            - <span style="color:#66d9ef">name</span>: FELIX_DEFAULTENDPOINTTOHOSTACTION
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;ACCEPT&#34;</span>
            <span style="color:#75715e"># Disable IPv6 on Kubernetes.</span>
            - <span style="color:#66d9ef">name</span>: FELIX_IPV6SUPPORT
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;false&#34;</span>
            <span style="color:#75715e"># Set Felix logging to &#34;info&#34;</span>
            - <span style="color:#66d9ef">name</span>: FELIX_LOGSEVERITYSCREEN
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;info&#34;</span>
            - <span style="color:#66d9ef">name</span>: FELIX_HEALTHENABLED
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;true&#34;</span>
          securityContext:
            <span style="color:#66d9ef">privileged</span>: <span style="color:#66d9ef">true</span>
          resources:
            requests:
              <span style="color:#66d9ef">cpu</span>: 250m
          livenessProbe:
            httpGet:
              <span style="color:#66d9ef">path</span>: /liveness
              <span style="color:#66d9ef">port</span>: <span style="color:#ae81ff">9099</span>
              <span style="color:#66d9ef">host</span>: localhost
            <span style="color:#66d9ef">periodSeconds</span>: <span style="color:#ae81ff">10</span>
            <span style="color:#66d9ef">initialDelaySeconds</span>: <span style="color:#ae81ff">10</span>
            <span style="color:#66d9ef">failureThreshold</span>: <span style="color:#ae81ff">6</span>
          readinessProbe:
            exec:
              command:
              - /bin/calico-node
              - -bird-ready
              - -felix-ready
            <span style="color:#66d9ef">periodSeconds</span>: <span style="color:#ae81ff">10</span>
          volumeMounts:
            - <span style="color:#66d9ef">mountPath</span>: /lib/modules
              <span style="color:#66d9ef">name</span>: lib-modules
              <span style="color:#66d9ef">readOnly</span>: <span style="color:#66d9ef">true</span>
            - <span style="color:#66d9ef">mountPath</span>: /run/xtables.lock
              <span style="color:#66d9ef">name</span>: xtables-lock
              <span style="color:#66d9ef">readOnly</span>: <span style="color:#66d9ef">false</span>
            - <span style="color:#66d9ef">mountPath</span>: /var/run/calico
              <span style="color:#66d9ef">name</span>: var-run-calico
              <span style="color:#66d9ef">readOnly</span>: <span style="color:#66d9ef">false</span>
            - <span style="color:#66d9ef">mountPath</span>: /var/lib/calico
              <span style="color:#66d9ef">name</span>: var-lib-calico
              <span style="color:#66d9ef">readOnly</span>: <span style="color:#66d9ef">false</span>
        <span style="color:#75715e"># This container installs the Calico CNI binaries</span>
        <span style="color:#75715e"># and CNI network config file on each node.</span>
        - <span style="color:#66d9ef">name</span>: install-cni
          <span style="color:#66d9ef">image</span>: quay.io/calico/cni:v3<span style="color:#ae81ff">.3</span><span style="color:#ae81ff">.2</span>
          <span style="color:#66d9ef">command</span>: [<span style="color:#e6db74">&#34;/install-cni.sh&#34;</span>]
          env:
            <span style="color:#75715e"># Name of the CNI config file to create.</span>
            - <span style="color:#66d9ef">name</span>: CNI_CONF_NAME
              <span style="color:#66d9ef">value</span>: <span style="color:#e6db74">&#34;10-calico.conflist&#34;</span>
            <span style="color:#75715e"># Set the hostname based on the k8s node name.</span>
            - <span style="color:#66d9ef">name</span>: KUBERNETES_NODE_NAME
              valueFrom:
                fieldRef:
                  <span style="color:#66d9ef">fieldPath</span>: spec.nodeName
            <span style="color:#75715e"># The CNI network config to install on each node.</span>
            - <span style="color:#66d9ef">name</span>: CNI_NETWORK_CONFIG
              valueFrom:
                configMapKeyRef:
                  <span style="color:#66d9ef">name</span>: calico-config
                  <span style="color:#66d9ef">key</span>: cni_network_config
            <span style="color:#75715e"># CNI MTU Config variable</span>
            - <span style="color:#66d9ef">name</span>: CNI_MTU
              valueFrom:
                configMapKeyRef:
                  <span style="color:#66d9ef">name</span>: calico-config
                  <span style="color:#66d9ef">key</span>: veth_mtu
          volumeMounts:
            - <span style="color:#66d9ef">mountPath</span>: /host/opt/cni/bin
              <span style="color:#66d9ef">name</span>: cni-bin-dir
            - <span style="color:#66d9ef">mountPath</span>: /host/etc/cni/net.d
              <span style="color:#66d9ef">name</span>: cni-net-dir
      volumes:
        <span style="color:#75715e"># Used by calico/node.</span>
        - <span style="color:#66d9ef">name</span>: lib-modules
          hostPath:
            <span style="color:#66d9ef">path</span>: /lib/modules
        - <span style="color:#66d9ef">name</span>: var-run-calico
          hostPath:
            <span style="color:#66d9ef">path</span>: /var/run/calico
        - <span style="color:#66d9ef">name</span>: var-lib-calico
          hostPath:
            <span style="color:#66d9ef">path</span>: /var/lib/calico
        - <span style="color:#66d9ef">name</span>: xtables-lock
          hostPath:
            <span style="color:#66d9ef">path</span>: /run/xtables.lock
            <span style="color:#66d9ef">type</span>: FileOrCreate
        <span style="color:#75715e"># Used to install CNI.</span>
        - <span style="color:#66d9ef">name</span>: cni-bin-dir
          hostPath:
            <span style="color:#66d9ef">path</span>: /opt/cni/bin
        - <span style="color:#66d9ef">name</span>: cni-net-dir
          hostPath:
            <span style="color:#66d9ef">path</span>: /etc/cni/net.d
---

<span style="color:#66d9ef">apiVersion</span>: v1
<span style="color:#66d9ef">kind</span>: ServiceAccount
metadata:
  <span style="color:#66d9ef">name</span>: calico-node
  <span style="color:#66d9ef">namespace</span>: kube-system

---

<span style="color:#75715e"># Create all the CustomResourceDefinitions needed for</span>
<span style="color:#75715e"># Calico policy and networking mode.</span>

<span style="color:#66d9ef">apiVersion</span>: apiextensions.k8s.io/v1beta1
<span style="color:#66d9ef">kind</span>: CustomResourceDefinition
metadata:
   <span style="color:#66d9ef">name</span>: felixconfigurations.crd.projectcalico.org
spec:
  <span style="color:#66d9ef">scope</span>: Cluster
  <span style="color:#66d9ef">group</span>: crd.projectcalico.org
  <span style="color:#66d9ef">version</span>: v1
  names:
    <span style="color:#66d9ef">kind</span>: FelixConfiguration
    <span style="color:#66d9ef">plural</span>: felixconfigurations
    <span style="color:#66d9ef">singular</span>: felixconfiguration
---

<span style="color:#66d9ef">apiVersion</span>: apiextensions.k8s.io/v1beta1
<span style="color:#66d9ef">kind</span>: CustomResourceDefinition
metadata:
  <span style="color:#66d9ef">name</span>: bgppeers.crd.projectcalico.org
spec:
  <span style="color:#66d9ef">scope</span>: Cluster
  <span style="color:#66d9ef">group</span>: crd.projectcalico.org
  <span style="color:#66d9ef">version</span>: v1
  names:
    <span style="color:#66d9ef">kind</span>: BGPPeer
    <span style="color:#66d9ef">plural</span>: bgppeers
    <span style="color:#66d9ef">singular</span>: bgppeer

---

<span style="color:#66d9ef">apiVersion</span>: apiextensions.k8s.io/v1beta1
<span style="color:#66d9ef">kind</span>: CustomResourceDefinition
metadata:
  <span style="color:#66d9ef">name</span>: bgpconfigurations.crd.projectcalico.org
spec:
  <span style="color:#66d9ef">scope</span>: Cluster
  <span style="color:#66d9ef">group</span>: crd.projectcalico.org
  <span style="color:#66d9ef">version</span>: v1
  names:
    <span style="color:#66d9ef">kind</span>: BGPConfiguration
    <span style="color:#66d9ef">plural</span>: bgpconfigurations
    <span style="color:#66d9ef">singular</span>: bgpconfiguration

---

<span style="color:#66d9ef">apiVersion</span>: apiextensions.k8s.io/v1beta1
<span style="color:#66d9ef">kind</span>: CustomResourceDefinition
metadata:
  <span style="color:#66d9ef">name</span>: ippools.crd.projectcalico.org
spec:
  <span style="color:#66d9ef">scope</span>: Cluster
  <span style="color:#66d9ef">group</span>: crd.projectcalico.org
  <span style="color:#66d9ef">version</span>: v1
  names:
    <span style="color:#66d9ef">kind</span>: IPPool
    <span style="color:#66d9ef">plural</span>: ippools
    <span style="color:#66d9ef">singular</span>: ippool

---

<span style="color:#66d9ef">apiVersion</span>: apiextensions.k8s.io/v1beta1
<span style="color:#66d9ef">kind</span>: CustomResourceDefinition
metadata:
  <span style="color:#66d9ef">name</span>: hostendpoints.crd.projectcalico.org
spec:
  <span style="color:#66d9ef">scope</span>: Cluster
  <span style="color:#66d9ef">group</span>: crd.projectcalico.org
  <span style="color:#66d9ef">version</span>: v1
  names:
    <span style="color:#66d9ef">kind</span>: HostEndpoint
    <span style="color:#66d9ef">plural</span>: hostendpoints
    <span style="color:#66d9ef">singular</span>: hostendpoint

---

<span style="color:#66d9ef">apiVersion</span>: apiextensions.k8s.io/v1beta1
<span style="color:#66d9ef">kind</span>: CustomResourceDefinition
metadata:
  <span style="color:#66d9ef">name</span>: clusterinformations.crd.projectcalico.org
spec:
  <span style="color:#66d9ef">scope</span>: Cluster
  <span style="color:#66d9ef">group</span>: crd.projectcalico.org
  <span style="color:#66d9ef">version</span>: v1
  names:
    <span style="color:#66d9ef">kind</span>: ClusterInformation
    <span style="color:#66d9ef">plural</span>: clusterinformations
    <span style="color:#66d9ef">singular</span>: clusterinformation

---

<span style="color:#66d9ef">apiVersion</span>: apiextensions.k8s.io/v1beta1
<span style="color:#66d9ef">kind</span>: CustomResourceDefinition
metadata:
  <span style="color:#66d9ef">name</span>: globalnetworkpolicies.crd.projectcalico.org
spec:
  <span style="color:#66d9ef">scope</span>: Cluster
  <span style="color:#66d9ef">group</span>: crd.projectcalico.org
  <span style="color:#66d9ef">version</span>: v1
  names:
    <span style="color:#66d9ef">kind</span>: GlobalNetworkPolicy
    <span style="color:#66d9ef">plural</span>: globalnetworkpolicies
    <span style="color:#66d9ef">singular</span>: globalnetworkpolicy

---

<span style="color:#66d9ef">apiVersion</span>: apiextensions.k8s.io/v1beta1
<span style="color:#66d9ef">kind</span>: CustomResourceDefinition
metadata:
  <span style="color:#66d9ef">name</span>: globalnetworksets.crd.projectcalico.org
spec:
  <span style="color:#66d9ef">scope</span>: Cluster
  <span style="color:#66d9ef">group</span>: crd.projectcalico.org
  <span style="color:#66d9ef">version</span>: v1
  names:
    <span style="color:#66d9ef">kind</span>: GlobalNetworkSet
    <span style="color:#66d9ef">plural</span>: globalnetworksets
    <span style="color:#66d9ef">singular</span>: globalnetworkset

---

<span style="color:#66d9ef">apiVersion</span>: apiextensions.k8s.io/v1beta1
<span style="color:#66d9ef">kind</span>: CustomResourceDefinition
metadata:
  <span style="color:#66d9ef">name</span>: networkpolicies.crd.projectcalico.org
spec:
  <span style="color:#66d9ef">scope</span>: Namespaced
  <span style="color:#66d9ef">group</span>: crd.projectcalico.org
  <span style="color:#66d9ef">version</span>: v1
  names:
    <span style="color:#66d9ef">kind</span>: NetworkPolicy
    <span style="color:#66d9ef">plural</span>: networkpolicies
    <span style="color:#66d9ef">singular</span>: networkpolicy
</code></pre></div><p>部署：</p>
<pre><code class="language-code" data-lang="code">kubectl apply -f calico-rbac.yaml 
kubectl apply -f calico.yaml
</code></pre><h2 id="部署kubernetes-nfs-client-provisioner">部署Kubernetes NFS-Client Provisioner</h2>
<h3 id="1-部署nfs-server">1. 部署nfs-server</h3>
<pre><code class="language-code" data-lang="code">apt install nfs-kernel-server
编辑/etc/exports文件添加需要共享目录，每个目录的设置独占一行，编写格式如下：
NFS共享目录路径 客户机IP或者名称(参数1,参数2,...,参数n)
例如：
/home *(ro,sync,insecure,no_root_squash)
/share 192.168.1.*(rw,sync,insecure,no_subtree_check,no_root_squash)

systemctl start nfs-kernel-server.service
systemctl start start nfs-server
exportfs -rv
showmount -e ##查看挂载
</code></pre><h3 id="2-创建静态pv">2. 创建静态pv</h3>
<p>nfs-pv.yaml</p>
<pre><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv
  labels:
    name: mynfs # name can be anything
spec:
  storageClassName: manual # same storage class as pvc
  capacity:
    storage: 200Mi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 10.170.0.3 # ip addres of nfs server
    path: &quot;/share&quot; # path to directory
</code></pre><h3 id="3-部署动态pv">3. 部署动态pv</h3>
<p>静态pv是需要创建pv，然后再创建pvc绑定pv，而动态pv只需要创建pvc即可，创建pv的过程由Provisioner完成。</p>
<p>3.1 准备nfs-server地址</p>
<p>3.2 下载<strong>Provisioner</strong> 文件</p>
<p>git地址：https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client/deploy</p>
<p>使用里面的deploy文件夹下面的内容。</p>
<p>3.3 部署rbac权限控制</p>
<p>修改文件<code>deploy/rbac.yaml</code>：</p>
<pre><code class="language-code" data-lang="code"># Set the subject of the RBAC objects to the current namespace where the provisioner is being deployed
$ NS=$(kubectl config get-contexts|grep -e &quot;^\*&quot; |awk '{print $5}')
$ NAMESPACE=${NS:-default}
$ sed -i'' &quot;s/namespace:.*/namespace: $NAMESPACE/g&quot; ./deploy/rbac.yaml ./deploy/deployment.yaml
$ kubectl create -f deploy/rbac.yaml
</code></pre><p>rbac.yaml</p>
<pre><code class="language-code" data-lang="code">apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumes&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;persistentvolumeclaims&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]
  - apiGroups: [&quot;storage.k8s.io&quot;]
    resources: [&quot;storageclasses&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
  - apiGroups: [&quot;&quot;]
    resources: [&quot;events&quot;]
    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
rules:
  - apiGroups: [&quot;&quot;]
    resources: [&quot;endpoints&quot;]
    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
</code></pre><p>3.4 配置<strong>Provisioner</strong></p>
<p>修改配置文件<code>deploy/deployment.yaml</code>：</p>
<pre><code class="language-code" data-lang="code">kind: Deployment
apiVersion: apps/v1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-client-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:latest
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME 
              value: fuseim.pri/ifs  ## 此处如果修改，则下文和后续class.yaml需要修改，不建议修改
            - name: NFS_SERVER
              value: &lt;YOUR NFS SERVER HOSTNAME&gt; ##nfs服务地址，本人此处填：10.170.0.3
            - name: NFS_PATH
              value: /var/nfs ## nfs服务挂载目录，和下文一致，/share
      volumes:
        - name: nfs-client-root
          nfs:
            server: &lt;YOUR NFS SERVER HOSTNAME&gt; ##nfs服务地址，本人此处填：10.170.0.3
            path: /var/nfs ##nfs挂载目录，和上文一致/share
</code></pre><p>修改配置<code>deploy/class.yaml</code>：</p>
<pre><code class="language-code" data-lang="code">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: fuseim.pri/ifs # 名称和上面的保持一致
parameters:
  archiveOnDelete: &quot;false&quot; # When set to &quot;false&quot; your PVs will not be archived
                           # by the provisioner upon deletion of the PVC.
</code></pre><p>3.5 测试</p>
<pre><code class="language-code" data-lang="code">kubectl create -f deploy/test-claim.yaml -f deploy/test-pod.yaml
</code></pre><p>使用<code>kubectl get pv,pvc</code>可以发现自动创建了pv。</p>
<h2 id="其他">其他</h2>
<pre><code class="language-code" data-lang="code">kubectl taint node xxx-nodename node-role.kubernetes.io/master-  #将 Master 也当作 Node 使用
</code></pre>
    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>作者:</span>
                <span>linjinbao66 </span>
                </p>
            
    </div>

    <div class="post-tags">
        
        <section>
                <a href="javascript:window.history.back();">返回</a></span> · 
                <span><a href="https://linjinbao.github.io">主页</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://linjinbao.github.io/2020/2020-10-14-k8s%E9%83%A8%E7%BD%B2traefik2.2/" class="prev" rel="prev" title="k8s部署traefik2.2"><i class="iconfont icon-left"></i>&nbsp;k8s部署traefik2.2</a>
         
        
        <a href="https://linjinbao.github.io/2020/2020-10-15-k8s-%E6%98%A0%E5%B0%84%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1/" class="next" rel="next" title="k8s 映射外部服务">k8s 映射外部服务&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
		  <div id="utter-container"></div>
    <script src="https://utteranc.es/client.js"
        repo= 'linjinbao/linjinbao.github.io'
        issue-term= 'title'
        label= 'utterance'
        theme= 'github-dark-orange'
        crossorigin="anonymous"
        async>
    </script>

    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2019 - 2021</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i> 
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="https://linjinbao.github.io">linjinbao66</a>  </span> 
         

         
		 
    </div>
</footer>












    
    
    <script src="/js/vendor_no_gallery.min.js" async=""></script>
    
  



     </div>
  </body>
</html>
