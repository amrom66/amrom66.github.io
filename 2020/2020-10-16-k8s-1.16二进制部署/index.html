<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="linjinbao66">
  <meta name="description" content="打工笔记">
  <meta name="keywords" content="博客 个人 笔记">
  
  <link rel="prev" href="https://linjinbao.github.io/2020/2020-10-16-k8s%E7%9A%84rbac%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/" />
  <link rel="next" href="https://linjinbao.github.io/2020/about/" />
  <link rel="canonical" href="https://linjinbao.github.io/2020/2020-10-16-k8s-1.16%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           k8s-1.16二进制部署 | 打工笔记
       
  </title>
  <meta name="title" content="k8s-1.16二进制部署 | 打工笔记">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/linjinbao.github.io"
    },
    "articleSection" : "posts",
    "name" : "k8s-1.16二进制部署",
    "headline" : "k8s-1.16二进制部署",
    "description" : "01. 初始化系统和全局变量 设置主机名 hostnamectl set-hostname node2 # 将 node2 替换为当前主机名\r如果 DNS 不支持主机名称解析，还需要在每台机器的 \/etc\/hosts 文件中添加主机名和 IP 的对应关系：\ncat \x26gt;\x26gt; \/etc\/hosts \x26lt;\x26lt;EOF\r10.",
    "inLanguage" : "en-us",
    "author" : "linjinbao66",
    "creator" : "linjinbao66",
    "publisher": "linjinbao66",
    "accountablePerson" : "linjinbao66",
    "copyrightHolder" : "linjinbao66",
    "copyrightYear" : "2020",
    "datePublished": "2020-10-16 00:00:00 \x2b0000 UTC",
    "dateModified" : "2020-10-16 00:00:00 \x2b0000 UTC",
    "url" : "https:\/\/linjinbao.github.io\/2020\/2020-10-16-k8s-1.16%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2\/",
    "wordCount" : "6704",
    "keywords" : [  "打工笔记"]
}
</script>

</head>

  


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-160203449-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://linjinbao.github.io">打工笔记</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">博客</a>
                
                <a class="menu-item" href="/categories/" title="">分类</a>
                
                <a class="menu-item" href="/tags/" title="">标签</a>
                
                <a class="menu-item" href="/2020/about/" title="关于">关于</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>
                <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;
                <a href="https://linjinbao.github.io">打工笔记</a>
            </div>
            <div class="menu-toggle">
                <svg t="1618556759902" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1299" width="16" height="16"><path d="M896 1024h-213.333333a128 128 0 0 1-128-128v-213.333333a128 128 0 0 1 128-128h213.333333a128 128 0 0 1 128 128v213.333333a128 128 0 0 1-128 128z m-213.333333-384a42.666667 42.666667 0 0 0-42.666667 42.666667v213.333333a42.666667 42.666667 0 0 0 42.666667 42.666667h213.333333a42.666667 42.666667 0 0 0 42.666667-42.666667v-213.333333a42.666667 42.666667 0 0 0-42.666667-42.666667z m-341.333334 384H128a128 128 0 0 1-128-128v-213.333333a128 128 0 0 1 128-128h213.333333a128 128 0 0 1 128 128v213.333333a128 128 0 0 1-128 128z m-213.333333-384a42.666667 42.666667 0 0 0-42.666667 42.666667v213.333333a42.666667 42.666667 0 0 0 42.666667 42.666667h213.333333a42.666667 42.666667 0 0 0 42.666667-42.666667v-213.333333a42.666667 42.666667 0 0 0-42.666667-42.666667z m768-170.666667h-213.333333a128 128 0 0 1-128-128V128a128 128 0 0 1 128-128h213.333333a128 128 0 0 1 128 128v213.333333a128 128 0 0 1-128 128z m-213.333333-384a42.666667 42.666667 0 0 0-42.666667 42.666667v213.333333a42.666667 42.666667 0 0 0 42.666667 42.666667h213.333333a42.666667 42.666667 0 0 0 42.666667-42.666667V128a42.666667 42.666667 0 0 0-42.666667-42.666667z m-341.333334 384H128a128 128 0 0 1-128-128V128a128 128 0 0 1 128-128h213.333333a128 128 0 0 1 128 128v213.333333a128 128 0 0 1-128 128zM128 85.333333a42.666667 42.666667 0 0 0-42.666667 42.666667v213.333333a42.666667 42.666667 0 0 0 42.666667 42.666667h213.333333a42.666667 42.666667 0 0 0 42.666667-42.666667V128a42.666667 42.666667 0 0 0-42.666667-42.666667z" p-id="1300"></path></svg>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">博客</a>
                
                <a class="menu-item" href="/categories/" title="">分类</a>
                
                <a class="menu-item" href="/tags/" title="">标签</a>
                
                <a class="menu-item" href="/2020/about/" title="关于">关于</a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">k8s-1.16二进制部署</h1>
        <div class="post-meta">
                作者： <a itemprop="name" href="https://linjinbao.github.io" rel="author">linjinbao66</a> 时间： 
                <span class="post-time">
                 <time datetime=16109-10-16 itemprop="datePublished">October 16, 2020</time>
                </span>
                分类： 
                
        </div>
    </header>
    <div class="post-content">
        

        

        
        
     
          
          
          

          
          
          

          <h1 id="01-初始化系统和全局变量">01. 初始化系统和全局变量</h1>
<h2 id="设置主机名">设置主机名</h2>
<pre><code class="language-code" data-lang="code">hostnamectl set-hostname node2 # 将 node2 替换为当前主机名
</code></pre><p>如果 DNS 不支持主机名称解析，还需要在每台机器的 <code>/etc/hosts</code> 文件中添加主机名和 IP 的对应关系：</p>
<pre><code class="language-code" data-lang="code">cat &gt;&gt; /etc/hosts &lt;&lt;EOF
10.20.250.23 node2
EOF
</code></pre><p>退出，重新登录 root 账号，可以看到主机名生效。</p>
<h2 id="添加节点信任关系">添加节点信任关系</h2>
<p>本操作只需要在 node2 节点上进行，设置 root 账户可以无密码登录<strong>所有节点</strong>：</p>
<pre><code>ssh-keygen -t rsa 
ssh-id-copy ....
</code></pre><h2 id="配置环境变量">配置环境变量</h2>
<p>由于计划将本次部署需要的所有执行文件放置在<code>/opt/k8s/bin</code>目录下，所以需要在<code>/etc/profile</code>中进行配置：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">export PATH<span style="color:#f92672">=</span>$PATH:/opt/k8s/bin
</code></pre></div><h2 id="安装依赖包">安装依赖包</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">yum install -y epel-release
yum install -y chrony conntrack ipvsadm ipset jq iptables curl sysstat libseccomp wget socat git
</code></pre></div><h2 id="关闭防火墙">关闭防火墙</h2>
<p>关闭防火墙，清理防火墙规则，设置默认转发策略：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">systemctl stop firewalld
systemctl disable firewalld
iptables -F <span style="color:#f92672">&amp;&amp;</span> iptables -X <span style="color:#f92672">&amp;&amp;</span> iptables -F -t nat <span style="color:#f92672">&amp;&amp;</span> iptables -X -t nat
iptables -P FORWARD ACCEPT
</code></pre></div><h2 id="关闭-swap-分区">关闭 swap 分区</h2>
<p>关闭 swap 分区，否则kubelet 会启动失败(可以设置 kubelet 启动参数 &ndash;fail-swap-on 为 false 关闭 swap 检查)：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">swapoff -a
sed -i <span style="color:#e6db74">&#39;/ swap / s/^\(.*\)$/#\1/g&#39;</span> /etc/fstab 
</code></pre></div><h2 id="关闭-selinux">关闭 SELinux</h2>
<p>关闭 SELinux，否则 kubelet 挂载目录时可能报错 <code>Permission denied</code>：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">setenforce 0
sed -i <span style="color:#e6db74">&#39;s/^SELINUX=.*/SELINUX=disabled/&#39;</span> /etc/selinux/config
</code></pre></div><h2 id="优化内核参数">优化内核参数</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat &gt; kubernetes.conf <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-iptables=1
</span><span style="color:#e6db74">net.bridge.bridge-nf-call-ip6tables=1
</span><span style="color:#e6db74">net.ipv4.ip_forward=1
</span><span style="color:#e6db74">net.ipv4.tcp_tw_recycle=0
</span><span style="color:#e6db74">net.ipv4.neigh.default.gc_thresh1=1024
</span><span style="color:#e6db74">net.ipv4.neigh.default.gc_thresh1=2048
</span><span style="color:#e6db74">net.ipv4.neigh.default.gc_thresh1=4096
</span><span style="color:#e6db74">vm.swappiness=0
</span><span style="color:#e6db74">vm.overcommit_memory=1
</span><span style="color:#e6db74">vm.panic_on_oom=0
</span><span style="color:#e6db74">fs.inotify.max_user_instances=8192
</span><span style="color:#e6db74">fs.inotify.max_user_watches=1048576
</span><span style="color:#e6db74">fs.file-max=52706963
</span><span style="color:#e6db74">fs.nr_open=52706963
</span><span style="color:#e6db74">net.ipv6.conf.all.disable_ipv6=1
</span><span style="color:#e6db74">net.netfilter.nf_conntrack_max=2310720
</span><span style="color:#e6db74">EOF</span>
cp kubernetes.conf  /etc/sysctl.d/kubernetes.conf
sysctl -p /etc/sysctl.d/kubernetes.conf
</code></pre></div><h2 id="设置系统时区">设置系统时区</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">timedatectl set-timezone Asia/Shanghai
</code></pre></div><h2 id="设置系统时钟同步">设置系统时钟同步</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">systemctl enable chronyd
systemctl start chronyd
<span style="color:#75715e"># 将当前的 UTC 时间写入硬件时钟</span>
timedatectl set-local-rtc 0

<span style="color:#75715e"># 重启依赖于系统时间的服务</span>
systemctl restart rsyslog 
systemctl restart crond
</code></pre></div><h2 id="关闭无关的服务">关闭无关的服务</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">systemctl stop postfix <span style="color:#f92672">&amp;&amp;</span> systemctl disable postfix
</code></pre></div><h2 id="创建相关目录">创建相关目录</h2>
<p>创建目录：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">mkdir -p /opt/k8s/<span style="color:#f92672">{</span>bin,work<span style="color:#f92672">}</span> /etc/<span style="color:#f92672">{</span>kubernetes,etcd<span style="color:#f92672">}</span>/cert
</code></pre></div><h2 id="分发集群配置参数脚本">分发集群配置参数脚本</h2>
<p>后续使用的环境变量都定义在文件 <code>environment.sh</code>中，请根据<strong>自己的机器、网络情况</strong>修改。然后拷贝到<strong>所有</strong>节点：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">source environment.sh <span style="color:#75715e"># 先修改，重要配置项都在这里</span>
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    scp environment.sh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>:/opt/k8s/bin/
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;chmod +x /opt/k8s/bin/*&#34;</span>
  <span style="color:#66d9ef">done</span>
</code></pre></div><h1 id="02-创建-ca-根证书和秘钥">02. 创建 CA 根证书和秘钥</h1>
<p>为确保安全，<code>kubernetes</code> 系统各组件需要使用 <code>x509</code> 证书对通信进行加密和认证。</p>
<p>CA (Certificate Authority) 是自签名的根证书，用来签名后续创建的其它证书。</p>
<p>CA 证书是集群所有节点共享的，<strong>只需要创建一次</strong>，后续用它签名其它所有证书。</p>
<p>本文档使用 <code>CloudFlare</code> 的 PKI 工具集 <a href="https://github.com/cloudflare/cfssl">cfssl</a> 创建所有证书。</p>
<h2 id="创建配置文件">创建配置文件</h2>
<p>CA 配置文件用于配置根证书的使用场景 (profile) 和具体参数 (usage，过期时间、服务端认证、客户端认证、加密等)：</p>
<pre><code>cd /opt/k8s/work
cat &gt; ca-config.json &lt;&lt;EOF
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;87600h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;876000h&quot;
      }
    }
  }
}
EOF
</code></pre><ul>
<li><code>signing</code>：表示该证书可用于签名其它证书（生成的 <code>ca.pem</code> 证书中 <code>CA=TRUE</code>）；</li>
<li><code>server auth</code>：表示 client 可以用该该证书对 server 提供的证书进行验证；</li>
<li><code>client auth</code>：表示 server 可以用该该证书对 client 提供的证书进行验证；</li>
<li><code>&quot;expiry&quot;: &quot;876000h&quot;</code>：证书有效期设置为 100 年；</li>
</ul>
<h2 id="创建证书签名请求文件">创建证书签名请求文件</h2>
<pre><code>cd /opt/k8s/work
cat &gt; ca-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;kubernetes-ca&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;opsnull&quot;
    }
  ],
  &quot;ca&quot;: {
    &quot;expiry&quot;: &quot;876000h&quot;
 }
}
EOF
</code></pre><ul>
<li><code>CN：Common Name</code>：kube-apiserver 从证书中提取该字段作为请求的<strong>用户名 (User Name)</strong>，浏览器使用该字段验证网站是否合法；</li>
<li><code>O：Organization</code>：kube-apiserver 从证书中提取该字段作为请求用户所属的<strong>组 (Group)</strong>；</li>
<li>kube-apiserver 将提取的 <code>User、Group</code> 作为 <code>RBAC</code> 授权的用户标识；</li>
</ul>
<p>注意：</p>
<ol>
<li>不同证书 csr 文件的 CN、C、ST、L、O、OU 组合必须不同，否则可能出现 <code>PEER'S CERTIFICATE HAS AN INVALID SIGNATURE</code> 错误；</li>
<li>后续创建证书的 csr 文件时，CN 都不相同（C、ST、L、O、OU 相同），以达到区分的目的；</li>
</ol>
<h2 id="生成-ca-证书和私钥">生成 CA 证书和私钥</h2>
<pre><code>cd /opt/k8s/work
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
ls ca*
</code></pre><h2 id="分发证书文件">分发证书文件</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/kubernetes/cert&quot;
    scp ca*.pem ca-config.json root@${node_ip}:/etc/kubernetes/cert
  done
</code></pre><h1 id="03-安装和配置-kubectl">03. 安装和配置 kubectl</h1>
<h2 id="创建-admin-证书和私钥">创建 admin 证书和私钥</h2>
<p>kubectl 使用 https 协议与 kube-apiserver 进行安全通信，kube-apiserver 对 kubectl 请求包含的证书进行认证和授权。</p>
<p>kubectl 后续用于集群管理，所以这里创建具有<strong>最高权限</strong>的 admin 证书。</p>
<p>创建证书签名请求：</p>
<pre><code>cd /opt/k8s/work
cat &gt; admin-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;admin&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;system:masters&quot;,
      &quot;OU&quot;: &quot;opsnull&quot;
    }
  ]
}
EOF
</code></pre><ul>
<li><code>O: system:masters</code>：kube-apiserver 收到使用该证书的客户端请求后，为请求添加组（Group）认证标识 <code>system:masters</code>；</li>
<li>预定义的 ClusterRoleBinding <code>cluster-admin</code> 将 Group <code>system:masters</code> 与 Role <code>cluster-admin</code> 绑定，该 Role 授予操作集群所需的<strong>最高</strong>权限；</li>
<li>该证书只会被 kubectl 当做 client 证书使用，所以 <code>hosts</code> 字段为空；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cd /opt/k8s/work
cfssl gencert -ca=/opt/k8s/work/ca.pem \
  -ca-key=/opt/k8s/work/ca-key.pem \
  -config=/opt/k8s/work/ca-config.json \
  -profile=kubernetes admin-csr.json | cfssljson -bare admin
ls admin*
</code></pre><ul>
<li>忽略警告消息 <code>[WARNING] This certificate lacks a &quot;hosts&quot; field.</code>；</li>
</ul>
<h2 id="创建-kubeconfig-文件">创建 kubeconfig 文件</h2>
<p>kubectl 使用 kubeconfig 文件访问 apiserver，该文件包含 kube-apiserver 的地址和认证信息（CA 证书和客户端证书）：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh

<span style="color:#75715e"># 设置集群参数</span>
kubectl config set-cluster kubernetes <span style="color:#ae81ff">\</span>
  --certificate-authority<span style="color:#f92672">=</span>/opt/k8s/work/ca.pem <span style="color:#ae81ff">\</span>
  --embed-certs<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\</span>
  --server<span style="color:#f92672">=</span>https://<span style="color:#e6db74">${</span>NODE_IPS[0]<span style="color:#e6db74">}</span>:6443 <span style="color:#ae81ff">\</span>
  --kubeconfig<span style="color:#f92672">=</span>kubectl.kubeconfig

<span style="color:#75715e"># 设置客户端认证参数</span>
kubectl config set-credentials admin <span style="color:#ae81ff">\</span>
  --client-certificate<span style="color:#f92672">=</span>/opt/k8s/work/admin.pem <span style="color:#ae81ff">\</span>
  --client-key<span style="color:#f92672">=</span>/opt/k8s/work/admin-key.pem <span style="color:#ae81ff">\</span>
  --embed-certs<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\</span>
  --kubeconfig<span style="color:#f92672">=</span>kubectl.kubeconfig

<span style="color:#75715e"># 设置上下文参数</span>
kubectl config set-context kubernetes <span style="color:#ae81ff">\</span>
  --cluster<span style="color:#f92672">=</span>kubernetes <span style="color:#ae81ff">\</span>
  --user<span style="color:#f92672">=</span>admin <span style="color:#ae81ff">\</span>
  --kubeconfig<span style="color:#f92672">=</span>kubectl.kubeconfig

<span style="color:#75715e"># 设置默认上下文</span>
kubectl config use-context kubernetes --kubeconfig<span style="color:#f92672">=</span>kubectl.kubeconfig
</code></pre></div><ul>
<li><code>--certificate-authority</code>：验证 kube-apiserver 证书的根证书；</li>
<li><code>--client-certificate</code>、<code>--client-key</code>：刚生成的 <code>admin</code> 证书和私钥，与 kube-apiserver https 通信时使用；</li>
<li><code>--embed-certs=true</code>：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(否则，写入的是证书文件路径，后续拷贝 kubeconfig 到其它机器时，还需要单独拷贝证书文件，不方便。)；</li>
<li><code>--server</code>：指定 kube-apiserver 的地址，这里指向第一个节点上的服务；</li>
</ul>
<h2 id="分发-kubeconfig-文件">分发 kubeconfig 文件</h2>
<p>分发到所有使用 <code>kubectl</code> 命令的节点：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;mkdir -p ~/.kube&#34;</span>
    scp kubectl.kubeconfig root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>:~/.kube/config
  <span style="color:#66d9ef">done</span>
</code></pre></div><h1 id="04-部署-etcd-集群">04. 部署 etcd 集群</h1>
<p>etcd 是基于 Raft 的分布式 KV 存储系统，由 CoreOS 开发，常用于服务发现、共享配置以及并发控制（如 leader 选举、分布式锁等）。</p>
<p>kubernetes 使用 etcd 集群持久化存储所有 API 对象、运行数据。</p>
<p>本文档介绍部署一个高可用 etcd 集群的步骤：</p>
<ul>
<li>下载和分发 etcd 二进制文件；</li>
<li>创建 etcd 集群各节点的 x509 证书，用于加密客户端(如 etcdctl) 与 etcd 集群、etcd 集群之间的通信；</li>
<li>创建 etcd 的 systemd unit 文件，配置服务参数；</li>
<li>检查集群工作状态；</li>
</ul>
<p>注意：</p>
<ol>
<li>flanneld 与本文档安装的 etcd v3.4.x 不兼容，如果要安装 flanneld（本文档使用 calio），则需要将 etcd <strong>降级到 v3.3.x 版本</strong>；</li>
</ol>
<h2 id="创建-etcd-证书和私钥">创建 etcd 证书和私钥</h2>
<p>创建证书签名请求：</p>
<pre><code>cd /opt/k8s/work
cat &gt; etcd-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;etcd&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;10.20.250.23&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;opsnull&quot;
    }
  ]
}
EOF
</code></pre><ul>
<li><code>hosts</code>：指定授权使用该证书的 etcd 节点 IP 列表，<strong>需要将 etcd 集群所有节点 IP 都列在其中</strong>；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cd /opt/k8s/work
cfssl gencert -ca=/opt/k8s/work/ca.pem \
    -ca-key=/opt/k8s/work/ca-key.pem \
    -config=/opt/k8s/work/ca-config.json \
    -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
ls etcd*pem
</code></pre><p>分发生成的证书和私钥到各 etcd 节点：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/etcd/cert&quot;
    scp etcd*.pem root@${node_ip}:/etc/etcd/cert/
  done
</code></pre><h2 id="创建-etcd-的-systemd-unit-模板文件">创建 etcd 的 systemd unit 模板文件</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; etcd.service.template &lt;&lt;EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=${ETCD_DATA_DIR}
ExecStart=/opt/k8s/bin/etcd \\
  --data-dir=${ETCD_DATA_DIR} \\
  --wal-dir=${ETCD_WAL_DIR} \\
  --name=##NODE_NAME## \\
  --cert-file=/etc/etcd/cert/etcd.pem \\
  --key-file=/etc/etcd/cert/etcd-key.pem \\
  --trusted-ca-file=/etc/kubernetes/cert/ca.pem \\
  --peer-cert-file=/etc/etcd/cert/etcd.pem \\
  --peer-key-file=/etc/etcd/cert/etcd-key.pem \\
  --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --listen-peer-urls=https://##NODE_IP##:2380 \\
  --initial-advertise-peer-urls=https://##NODE_IP##:2380 \\
  --listen-client-urls=https://##NODE_IP##:2379,http://127.0.0.1:2379 \\
  --advertise-client-urls=https://##NODE_IP##:2379 \\
  --initial-cluster-token=etcd-cluster-0 \\
  --initial-cluster=${ETCD_NODES} \\
  --initial-cluster-state=new \\
  --auto-compaction-mode=periodic \\
  --auto-compaction-retention=1 \\
  --max-request-bytes=33554432 \\
  --quota-backend-bytes=6442450944 \\
  --heartbeat-interval=250 \\
  --election-timeout=2000
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre><ul>
<li><code>WorkingDirectory</code>、<code>--data-dir</code>：指定工作目录和数据目录为 <code>${ETCD_DATA_DIR}</code>，需在启动服务前创建这个目录；</li>
<li><code>--wal-dir</code>：指定 wal 目录，为了提高性能，一般使用 SSD 或者和 <code>--data-dir</code> 不同的磁盘；</li>
<li><code>--name</code>：指定节点名称，当 <code>--initial-cluster-state</code> 值为 <code>new</code> 时，<code>--name</code> 的参数值必须位于 <code>--initial-cluster</code> 列表中；</li>
<li><code>--cert-file</code>、<code>--key-file</code>：etcd server 与 client 通信时使用的证书和私钥；</li>
<li><code>--trusted-ca-file</code>：签名 client 证书的 CA 证书，用于验证 client 证书；</li>
<li><code>--peer-cert-file</code>、<code>--peer-key-file</code>：etcd 与 peer 通信使用的证书和私钥；</li>
<li><code>--peer-trusted-ca-file</code>：签名 peer 证书的 CA 证书，用于验证 peer 证书；</li>
</ul>
<h2 id="为各节点创建和分发-etcd-systemd-unit-文件">为各节点创建和分发 etcd systemd unit 文件</h2>
<p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; ${#NODE_IPS[@]}; i++ ))
  do
    sed -e &quot;s/##NODE_NAME##/${NODE_NAMES[i]}/&quot; -e &quot;s/##NODE_IP##/${NODE_IPS[i]}/&quot; etcd.service.template &gt; etcd-${NODE_IPS[i]}.service 
  done
ls *.service
</code></pre><ul>
<li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li>
</ul>
<p>分发生成的 systemd unit 文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp etcd-${node_ip}.service root@${node_ip}:/etc/systemd/system/etcd.service
  done
</code></pre><h2 id="启动-etcd-服务">启动 etcd 服务</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p ${ETCD_DATA_DIR} ${ETCD_WAL_DIR}&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl restart etcd &quot; &amp;
  done
</code></pre><ul>
<li>必须先创建 etcd 数据目录和工作目录;</li>
<li>etcd 进程首次启动时会等待其它节点的 etcd 加入集群，命令 <code>systemctl start etcd</code> 会卡住一段时间，为正常现象；</li>
</ul>
<h1 id="05-部署-kube-apiserver">05 部署 kube-apiserver</h1>
<h2 id="创建-kubernetes-master-证书和私钥">创建 kubernetes-master 证书和私钥</h2>
<p>创建证书签名请求：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; kubernetes-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;kubernetes-master&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;10.20.250.23&quot;,
    &quot;${CLUSTER_KUBERNETES_SVC_IP}&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local.&quot;,
    &quot;kubernetes.default.svc.${CLUSTER_DNS_DOMAIN}.&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;opsnull&quot;
    }
  ]
}
EOF
</code></pre><ul>
<li>hosts 字段指定授权使用该证书的 <strong>IP 和域名列表</strong>，这里列出了 master 节点 IP、kubernetes 服务的 IP 和域名；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/opt/k8s/work/ca.pem \
  -ca-key=/opt/k8s/work/ca-key.pem \
  -config=/opt/k8s/work/ca-config.json \
  -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
ls kubernetes*pem
</code></pre><p>将生成的证书和私钥文件拷贝到所有 master 节点：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/kubernetes/cert&quot;
    scp kubernetes*.pem root@${node_ip}:/etc/kubernetes/cert/
  done
</code></pre><h2 id="创建加密配置文件">创建加密配置文件</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; encryption-config.yaml &lt;&lt;EOF
kind: EncryptionConfig
apiVersion: v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: ${ENCRYPTION_KEY}
      - identity: {}
EOF
</code></pre><p>将加密配置文件拷贝到 master 节点的 <code>/etc/kubernetes</code> 目录下：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp encryption-config.yaml root@${node_ip}:/etc/kubernetes/
  done
</code></pre><h2 id="创建审计策略文件">创建审计策略文件</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; audit-policy.yaml &lt;&lt;EOF
apiVersion: audit.k8s.io/v1beta1
kind: Policy
rules:
  # The following requests were manually identified as high-volume and low-risk, so drop them.
  - level: None
    resources:
      - group: &quot;&quot;
        resources:
          - endpoints
          - services
          - services/status
    users:
      - 'system:kube-proxy'
    verbs:
      - watch

  - level: None
    resources:
      - group: &quot;&quot;
        resources:
          - nodes
          - nodes/status
    userGroups:
      - 'system:nodes'
    verbs:
      - get

  - level: None
    namespaces:
      - kube-system
    resources:
      - group: &quot;&quot;
        resources:
          - endpoints
    users:
      - 'system:kube-controller-manager'
      - 'system:kube-scheduler'
      - 'system:serviceaccount:kube-system:endpoint-controller'
    verbs:
      - get
      - update

  - level: None
    resources:
      - group: &quot;&quot;
        resources:
          - namespaces
          - namespaces/status
          - namespaces/finalize
    users:
      - 'system:apiserver'
    verbs:
      - get

  # Don't log HPA fetching metrics.
  - level: None
    resources:
      - group: metrics.k8s.io
    users:
      - 'system:kube-controller-manager'
    verbs:
      - get
      - list

  # Don't log these read-only URLs.
  - level: None
    nonResourceURLs:
      - '/healthz*'
      - /version
      - '/swagger*'

  # Don't log events requests.
  - level: None
    resources:
      - group: &quot;&quot;
        resources:
          - events

  # node and pod status calls from nodes are high-volume and can be large, don't log responses
  # for expected updates from nodes
  - level: Request
    omitStages:
      - RequestReceived
    resources:
      - group: &quot;&quot;
        resources:
          - nodes/status
          - pods/status
    users:
      - kubelet
      - 'system:node-problem-detector'
      - 'system:serviceaccount:kube-system:node-problem-detector'
    verbs:
      - update
      - patch

  - level: Request
    omitStages:
      - RequestReceived
    resources:
      - group: &quot;&quot;
        resources:
          - nodes/status
          - pods/status
    userGroups:
      - 'system:nodes'
    verbs:
      - update
      - patch

  # deletecollection calls can be large, don't log responses for expected namespace deletions
  - level: Request
    omitStages:
      - RequestReceived
    users:
      - 'system:serviceaccount:kube-system:namespace-controller'
    verbs:
      - deletecollection

  # Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data,
  # so only log at the Metadata level.
  - level: Metadata
    omitStages:
      - RequestReceived
    resources:
      - group: &quot;&quot;
        resources:
          - secrets
          - configmaps
      - group: authentication.k8s.io
        resources:
          - tokenreviews
  # Get repsonses can be large; skip them.
  - level: Request
    omitStages:
      - RequestReceived
    resources:
      - group: &quot;&quot;
      - group: admissionregistration.k8s.io
      - group: apiextensions.k8s.io
      - group: apiregistration.k8s.io
      - group: apps
      - group: authentication.k8s.io
      - group: authorization.k8s.io
      - group: autoscaling
      - group: batch
      - group: certificates.k8s.io
      - group: extensions
      - group: metrics.k8s.io
      - group: networking.k8s.io
      - group: policy
      - group: rbac.authorization.k8s.io
      - group: scheduling.k8s.io
      - group: settings.k8s.io
      - group: storage.k8s.io
    verbs:
      - get
      - list
      - watch

  # Default level for known APIs
  - level: RequestResponse
    omitStages:
      - RequestReceived
    resources:
      - group: &quot;&quot;
      - group: admissionregistration.k8s.io
      - group: apiextensions.k8s.io
      - group: apiregistration.k8s.io
      - group: apps
      - group: authentication.k8s.io
      - group: authorization.k8s.io
      - group: autoscaling
      - group: batch
      - group: certificates.k8s.io
      - group: extensions
      - group: metrics.k8s.io
      - group: networking.k8s.io
      - group: policy
      - group: rbac.authorization.k8s.io
      - group: scheduling.k8s.io
      - group: settings.k8s.io
      - group: storage.k8s.io
      
  # Default level for all other requests.
  - level: Metadata
    omitStages:
      - RequestReceived
EOF
</code></pre><p>分发审计策略文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp audit-policy.yaml root@${node_ip}:/etc/kubernetes/audit-policy.yaml
  done
</code></pre><h2 id="创建后续访问-metrics-server-或-kube-prometheus-使用的证书">创建后续访问 metrics-server 或 kube-prometheus 使用的证书</h2>
<p>创建证书签名请求:</p>
<pre><code>cd /opt/k8s/work
cat &gt; proxy-client-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;aggregator&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;opsnull&quot;
    }
  ]
}
EOF
</code></pre><ul>
<li>CN 名称需要位于 kube-apiserver 的 <code>--requestheader-allowed-names</code> 参数中，否则后续访问 metrics 时会提示权限不足。</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \
  -ca-key=/etc/kubernetes/cert/ca-key.pem  \
  -config=/etc/kubernetes/cert/ca-config.json  \
  -profile=kubernetes proxy-client-csr.json | cfssljson -bare proxy-client
ls proxy-client*.pem
</code></pre><p>将生成的证书和私钥文件拷贝到所有 master 节点：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp proxy-client*.pem root@${node_ip}:/etc/kubernetes/cert/
  done
</code></pre><h2 id="创建-kube-apiserver-systemd-unit-模板文件">创建 kube-apiserver systemd unit 模板文件</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; kube-apiserver.service.template &lt;&lt;EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=${K8S_DIR}/kube-apiserver
ExecStart=/opt/k8s/bin/kube-apiserver \\
  --advertise-address=##NODE_IP## \\
  --default-not-ready-toleration-seconds=360 \\
  --default-unreachable-toleration-seconds=360 \\
  --feature-gates=DynamicAuditing=true \\
  --max-mutating-requests-inflight=2000 \\
  --max-requests-inflight=4000 \\
  --default-watch-cache-size=200 \\
  --delete-collection-workers=2 \\
  --encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\
  --etcd-cafile=/etc/kubernetes/cert/ca.pem \\
  --etcd-certfile=/etc/kubernetes/cert/kubernetes.pem \\
  --etcd-keyfile=/etc/kubernetes/cert/kubernetes-key.pem \\
  --etcd-servers=${ETCD_ENDPOINTS} \\
  --bind-address=##NODE_IP## \\
  --secure-port=6443 \\
  --tls-cert-file=/etc/kubernetes/cert/kubernetes.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kubernetes-key.pem \\
  --insecure-port=0 \\
  --audit-dynamic-configuration \\
  --audit-log-maxage=15 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-truncate-enabled \\
  --audit-log-path=${K8S_DIR}/kube-apiserver/audit.log \\
  --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\
  --profiling \\
  --anonymous-auth=false \\
  --client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --enable-bootstrap-token-auth \\
  --requestheader-allowed-names=&quot;aggregator&quot; \\
  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --service-account-key-file=/etc/kubernetes/cert/ca.pem \\
  --authorization-mode=Node,RBAC \\
  --runtime-config=api/all=true \\
  --enable-admission-plugins=NodeRestriction \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --event-ttl=168h \\
  --kubelet-certificate-authority=/etc/kubernetes/cert/ca.pem \\
  --kubelet-client-certificate=/etc/kubernetes/cert/kubernetes.pem \\
  --kubelet-client-key=/etc/kubernetes/cert/kubernetes-key.pem \\
  --kubelet-https=true \\
  --kubelet-timeout=10s \\
  --proxy-client-cert-file=/etc/kubernetes/cert/proxy-client.pem \\
  --proxy-client-key-file=/etc/kubernetes/cert/proxy-client-key.pem \\
  --service-cluster-ip-range=${SERVICE_CIDR} \\
  --service-node-port-range=${NODE_PORT_RANGE} \\
  --logtostderr=true \\
  --v=2
Restart=on-failure
RestartSec=10
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre><ul>
<li><code>--advertise-address</code>：apiserver 对外通告的 IP（kubernetes 服务后端节点 IP）；</li>
<li><code>--default-*-toleration-seconds</code>：设置节点异常相关的阈值；</li>
<li><code>--max-*-requests-inflight</code>：请求相关的最大阈值；</li>
<li><code>--etcd-*</code>：访问 etcd 的证书和 etcd 服务器地址；</li>
<li><code>--bind-address</code>： https 监听的 IP，不能为 <code>127.0.0.1</code>，否则外界不能访问它的安全端口 6443；</li>
<li><code>--secret-port</code>：https 监听端口；</li>
<li><code>--insecure-port=0</code>：关闭监听 http 非安全端口(8080)；</li>
<li><code>--tls-*-file</code>：指定 apiserver 使用的证书、私钥和 CA 文件；</li>
<li><code>--audit-*</code>：配置审计策略和审计日志文件相关的参数；</li>
<li><code>--client-ca-file</code>：验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy 等)请求所带的证书；</li>
<li><code>--enable-bootstrap-token-auth</code>：启用 kubelet bootstrap 的 token 认证；</li>
<li><code>--requestheader-*</code>：kube-apiserver 的 aggregator layer 相关的配置参数，proxy-client &amp; HPA 需要使用；</li>
<li><code>--requestheader-client-ca-file</code>：用于签名 <code>--proxy-client-cert-file</code> 和 <code>--proxy-client-key-file</code> 指定的证书；在启用了 metric aggregator 时使用；</li>
<li><code>--requestheader-allowed-names</code>：不能为空，值为逗号分割的 <code>--proxy-client-cert-file</code> 证书的 CN 名称，这里设置为 &ldquo;aggregator&rdquo;；</li>
<li><code>--service-account-key-file</code>：签名 ServiceAccount Token 的公钥文件，kube-controller-manager 的 <code>--service-account-private-key-file</code> 指定私钥文件，两者配对使用；</li>
<li><code>--runtime-config=api/all=true</code>： 启用所有版本的 APIs，如 autoscaling/v2alpha1；</li>
<li><code>--authorization-mode=Node,RBAC</code>、<code>--anonymous-auth=false</code>： 开启 Node 和 RBAC 授权模式，拒绝未授权的请求；</li>
<li><code>--enable-admission-plugins</code>：启用一些默认关闭的 plugins；</li>
<li><code>--allow-privileged</code>：运行执行 privileged 权限的容器；</li>
<li><code>--apiserver-count=3</code>：指定 apiserver 实例的数量；</li>
<li><code>--event-ttl</code>：指定 events 的保存时间；</li>
<li><code>--kubelet-*</code>：如果指定，则使用 https 访问 kubelet APIs；需要为证书对应的用户(上面 kubernetes*.pem 证书的用户为 kubernetes) 用户定义 RBAC 规则，否则访问 kubelet API 时提示未授权；</li>
<li><code>--proxy-client-*</code>：apiserver 访问 metrics-server 使用的证书；</li>
<li><code>--service-cluster-ip-range</code>： 指定 Service Cluster IP 地址段；</li>
<li><code>--service-node-port-range</code>： 指定 NodePort 的端口范围；</li>
</ul>
<p>如果 kube-apiserver 机器<strong>没有</strong>运行 kube-proxy，则还需要添加 <code>--enable-aggregator-routing=true</code> 参数；</p>
<p>关于 <code>--requestheader-XXX</code> 相关参数，参考：</p>
<ul>
<li><a href="https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md">https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md</a></li>
<li><a href="https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/">https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/</a></li>
</ul>
<p>注意：</p>
<ol>
<li><code>--requestheader-client-ca-file</code> 指定的 CA 证书，必须具有 <code>client auth and server auth</code>；</li>
<li>如果 <code>--requestheader-allowed-names</code> 不为空,且 <code>--proxy-client-cert-file</code> 证书的 CN 名称不在 allowed-names 中，则后续查看 node 或 pods 的 metrics 失败，提示：</li>
</ol>
<pre><code>$ kubectl top nodes
Error from server (Forbidden): nodes.metrics.k8s.io is forbidden: User &quot;aggregator&quot; cannot list resource &quot;nodes&quot; in API group &quot;metrics.k8s.io&quot; at the cluster scope
</code></pre><h2 id="为各节点创建和分发-kube-apiserver-systemd-unit-文件">为各节点创建和分发 kube-apiserver systemd unit 文件</h2>
<p>替换模板文件中的变量，为各节点生成 systemd unit 文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; ${#NODE_IPS[@]}; i++ ))
  do
    sed -e &quot;s/##NODE_NAME##/${NODE_NAMES[i]}/&quot; -e &quot;s/##NODE_IP##/${NODE_IPS[i]}/&quot; kube-apiserver.service.template &gt; kube-apiserver-${NODE_IPS[i]}.service 
  done
ls kube-apiserver*.service
</code></pre><ul>
<li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li>
</ul>
<p>分发生成的 systemd unit 文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-apiserver-${node_ip}.service root@${node_ip}:/etc/systemd/system/kube-apiserver.service
  done
</code></pre><h2 id="启动-kube-apiserver-服务">启动 kube-apiserver 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p ${K8S_DIR}/kube-apiserver&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl restart kube-apiserver&quot;
  done
</code></pre><h2 id="检查-kube-apiserver-运行状态">检查 kube-apiserver 运行状态</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl status kube-apiserver |grep 'Active:'&quot;
  done
</code></pre><p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u kube-apiserver
</code></pre><h2 id="检查集群状态">检查集群状态</h2>
<pre><code>$ kubectl cluster-info
Kubernetes master is running at https://172.27.138.251:6443

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

$ kubectl get all --all-namespaces
NAMESPACE   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
default     service/kubernetes   ClusterIP   10.254.0.1   &lt;none&gt;        443/TCP   3m53s

$ kubectl get componentstatuses
NAME                 AGE
controller-manager   &lt;unknown&gt;
scheduler            &lt;unknown&gt;
etcd-0               &lt;unknown&gt;
etcd-2               &lt;unknown&gt;
etcd-1               &lt;unknown&gt;
</code></pre><ul>
<li>Kubernetes 1.16.6 存在 Bugs 导致返回结果一直为 <code>&lt;unknown&gt;</code>，但 <code>kubectl get cs -o yaml</code> 可以返回正确结果；</li>
</ul>
<h2 id="检查-kube-apiserver-监听的端口">检查 kube-apiserver 监听的端口</h2>
<pre><code>$ sudo netstat -lnpt|grep kube
tcp        0      0 172.27.138.251:6443     0.0.0.0:*               LISTEN      101442/kube-apiserv
</code></pre><ul>
<li>6443: 接收 https 请求的安全端口，对所有请求做认证和授权；</li>
<li>由于关闭了非安全端口，故没有监听 8080；</li>
</ul>
<h1 id="06-部署kube-controller-manager">06 部署kube-controller-manager</h1>
<h2 id="创建-kube-controller-manager-证书和私钥">创建 kube-controller-manager 证书和私钥</h2>
<p>创建证书签名请求：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
cat &gt; kube-controller-manager-csr.json <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">{
</span><span style="color:#e6db74">    &#34;CN&#34;: &#34;system:kube-controller-manager&#34;,
</span><span style="color:#e6db74">    &#34;key&#34;: {
</span><span style="color:#e6db74">        &#34;algo&#34;: &#34;rsa&#34;,
</span><span style="color:#e6db74">        &#34;size&#34;: 2048
</span><span style="color:#e6db74">    },
</span><span style="color:#e6db74">    &#34;hosts&#34;: [
</span><span style="color:#e6db74">      &#34;127.0.0.1&#34;,
</span><span style="color:#e6db74">      &#34;10.20.250.23&#34;    
</span><span style="color:#e6db74">    ],
</span><span style="color:#e6db74">    &#34;names&#34;: [
</span><span style="color:#e6db74">      {
</span><span style="color:#e6db74">        &#34;C&#34;: &#34;CN&#34;,
</span><span style="color:#e6db74">        &#34;ST&#34;: &#34;BeiJing&#34;,
</span><span style="color:#e6db74">        &#34;L&#34;: &#34;BeiJing&#34;,
</span><span style="color:#e6db74">        &#34;O&#34;: &#34;system:kube-controller-manager&#34;,
</span><span style="color:#e6db74">        &#34;OU&#34;: &#34;opsnull&#34;
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    ]
</span><span style="color:#e6db74">}
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><ul>
<li>hosts 列表包含<strong>所有</strong> kube-controller-manager 节点 IP；</li>
<li>CN 和 O 均为 <code>system:kube-controller-manager</code>，kubernetes 内置的 ClusterRoleBindings <code>system:kube-controller-manager</code> 赋予 kube-controller-manager 工作所需的权限。</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cd /opt/k8s/work
cfssl gencert -ca=/opt/k8s/work/ca.pem \
  -ca-key=/opt/k8s/work/ca-key.pem \
  -config=/opt/k8s/work/ca-config.json \
  -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
ls kube-controller-manager*pem
</code></pre><p>将生成的证书和私钥分发到所有 master 节点：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-controller-manager*.pem root@${node_ip}:/etc/kubernetes/cert/
  done
</code></pre><h2 id="创建和分发-kubeconfig-文件">创建和分发 kubeconfig 文件</h2>
<p>kube-controller-manager 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-controller-manager 证书等信息：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/k8s/work/ca.pem \
  --embed-certs=true \
  --server=&quot;https://##NODE_IP##:6443&quot; \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.pem \
  --client-key=kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context system:kube-controller-manager \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig
</code></pre><ul>
<li>kube-controller-manager 与 kube-apiserver 混布，故直接通过<strong>节点 IP</strong> 访问 kube-apiserver；</li>
</ul>
<p>分发 kubeconfig 到所有 master 节点：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    sed -e &quot;s/##NODE_IP##/${node_ip}/&quot; kube-controller-manager.kubeconfig &gt; kube-controller-manager-${node_ip}.kubeconfig
    scp kube-controller-manager-${node_ip}.kubeconfig root@${node_ip}:/etc/kubernetes/kube-controller-manager.kubeconfig
  done
</code></pre><h2 id="创建-kube-controller-manager-systemd-unit-模板文件">创建 kube-controller-manager systemd unit 模板文件</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; kube-controller-manager.service.template &lt;&lt;EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
WorkingDirectory=${K8S_DIR}/kube-controller-manager
ExecStart=/opt/k8s/bin/kube-controller-manager \\
  --profiling \\
  --cluster-name=kubernetes \\
  --controllers=*,bootstrapsigner,tokencleaner \\
  --kube-api-qps=1000 \\
  --kube-api-burst=2000 \\
  --leader-elect \\
  --use-service-account-credentials\\
  --concurrent-service-syncs=2 \\
  --bind-address=##NODE_IP## \\
  --secure-port=10252 \\
  --tls-cert-file=/etc/kubernetes/cert/kube-controller-manager.pem \\
  --tls-private-key-file=/etc/kubernetes/cert/kube-controller-manager-key.pem \\
  --port=0 \\
  --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\
  --client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --requestheader-allowed-names=&quot;aggregator&quot; \\
  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\
  --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\
  --requestheader-group-headers=X-Remote-Group \\
  --requestheader-username-headers=X-Remote-User \\
  --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\
  --cluster-signing-cert-file=/etc/kubernetes/cert/ca.pem \\
  --cluster-signing-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --experimental-cluster-signing-duration=876000h \\
  --horizontal-pod-autoscaler-sync-period=10s \\
  --concurrent-deployment-syncs=10 \\
  --concurrent-gc-syncs=30 \\
  --node-cidr-mask-size=24 \\
  --service-cluster-ip-range=${SERVICE_CIDR} \\
  --pod-eviction-timeout=6m \\
  --terminated-pod-gc-threshold=10000 \\
  --root-ca-file=/etc/kubernetes/cert/ca.pem \\
  --service-account-private-key-file=/etc/kubernetes/cert/ca-key.pem \\
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\
  --logtostderr=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
</code></pre><ul>
<li><code>--port=0</code>：关闭监听非安全端口（http），同时 <code>--address</code> 参数无效，<code>--bind-address</code> 参数有效；</li>
<li><code>--secure-port=10252</code>、<code>--bind-address=0.0.0.0</code>: 在所有网络接口监听 10252 端口的 https /metrics 请求；</li>
<li><code>--kubeconfig</code>：指定 kubeconfig 文件路径，kube-controller-manager 使用它连接和验证 kube-apiserver；</li>
<li><code>--authentication-kubeconfig</code> 和 <code>--authorization-kubeconfig</code>：kube-controller-manager 使用它连接 apiserver，对 client 的请求进行认证和授权。<code>kube-controller-manager</code> 不再使用 <code>--tls-ca-file</code> 对请求 https metrics 的 Client 证书进行校验。如果没有配置这两个 kubeconfig 参数，则 client 连接 kube-controller-manager https 端口的请求会被拒绝(提示权限不足)。</li>
<li><code>--cluster-signing-*-file</code>：签名 TLS Bootstrap 创建的证书；</li>
<li><code>--experimental-cluster-signing-duration</code>：指定 TLS Bootstrap 证书的有效期；</li>
<li><code>--root-ca-file</code>：放置到容器 ServiceAccount 中的 CA 证书，用来对 kube-apiserver 的证书进行校验；</li>
<li><code>--service-account-private-key-file</code>：签名 ServiceAccount 中 Token 的私钥文件，必须和 kube-apiserver 的 <code>--service-account-key-file</code> 指定的公钥文件配对使用；</li>
<li><code>--service-cluster-ip-range</code> ：指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致；</li>
<li><code>--leader-elect=true</code>：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li>
<li><code>--controllers=*,bootstrapsigner,tokencleaner</code>：启用的控制器列表，tokencleaner 用于自动清理过期的 Bootstrap token；</li>
<li><code>--horizontal-pod-autoscaler-*</code>：custom metrics 相关参数，支持 autoscaling/v2alpha1；</li>
<li><code>--tls-cert-file</code>、<code>--tls-private-key-file</code>：使用 https 输出 metrics 时使用的 Server 证书和秘钥；</li>
<li><code>--use-service-account-credentials=true</code>: kube-controller-manager 中各 controller 使用 serviceaccount 访问 kube-apiserver；</li>
</ul>
<h2 id="为各节点创建和分发-kube-controller-mananger-systemd-unit-文件">为各节点创建和分发 kube-controller-mananger systemd unit 文件</h2>
<p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> <span style="color:#f92672">(</span><span style="color:#f92672">(</span> i<span style="color:#f92672">=</span>0; i &lt; 3; i++ <span style="color:#f92672">)</span><span style="color:#f92672">)</span>
  <span style="color:#66d9ef">do</span>
    sed -e <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">s/##NODE_NAME##/</span><span style="color:#e6db74">${</span>NODE_NAMES[i]<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">&#34;</span> -e <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">s/##NODE_IP##/</span><span style="color:#e6db74">${</span>NODE_IPS[i]<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">&#34;</span> kube-controller-manager.service.template &gt; kube-controller-manager-<span style="color:#e6db74">${</span>NODE_IPS[i]<span style="color:#e6db74">}</span>.service 
  <span style="color:#66d9ef">done</span>
ls kube-controller-manager*.service
</code></pre></div><p>分发到所有 master 节点：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-controller-manager-${node_ip}.service root@${node_ip}:/etc/systemd/system/kube-controller-manager.service
  done
</code></pre><h2 id="启动-kube-controller-manager-服务">启动 kube-controller-manager 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p ${K8S_DIR}/kube-controller-manager&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl restart kube-controller-manager&quot;
  done
</code></pre><h2 id="检查服务运行状态">检查服务运行状态</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;systemctl status kube-controller-manager|grep Active&#34;</span>
  <span style="color:#66d9ef">done</span>
</code></pre></div><p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">journalctl -u kube-controller-manager
</code></pre></div><p>kube-controller-manager 监听 10252 端口，接收 https 请求：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ sudo netstat -lnpt | grep kube-cont
tcp        <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span> 172.27.138.251:10252    0.0.0.0:*               LISTEN      108977/kube-control
</code></pre></div><h1 id="07-部署kube-scheduler">07 部署kube-scheduler</h1>
<h2 id="创建-kube-scheduler-证书和私钥">创建 kube-scheduler 证书和私钥</h2>
<p>创建证书签名请求：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
cat &gt; kube-scheduler-csr.json <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">{
</span><span style="color:#e6db74">    &#34;CN&#34;: &#34;system:kube-scheduler&#34;,
</span><span style="color:#e6db74">    &#34;hosts&#34;: [
</span><span style="color:#e6db74">      &#34;127.0.0.1&#34;,
</span><span style="color:#e6db74">      &#34;10.20.250.23&#34;
</span><span style="color:#e6db74">    ],
</span><span style="color:#e6db74">    &#34;key&#34;: {
</span><span style="color:#e6db74">        &#34;algo&#34;: &#34;rsa&#34;,
</span><span style="color:#e6db74">        &#34;size&#34;: 2048
</span><span style="color:#e6db74">    },
</span><span style="color:#e6db74">    &#34;names&#34;: [
</span><span style="color:#e6db74">      {
</span><span style="color:#e6db74">        &#34;C&#34;: &#34;CN&#34;,
</span><span style="color:#e6db74">        &#34;ST&#34;: &#34;BeiJing&#34;,
</span><span style="color:#e6db74">        &#34;L&#34;: &#34;BeiJing&#34;,
</span><span style="color:#e6db74">        &#34;O&#34;: &#34;system:kube-scheduler&#34;,
</span><span style="color:#e6db74">        &#34;OU&#34;: &#34;opsnull&#34;
</span><span style="color:#e6db74">      }
</span><span style="color:#e6db74">    ]
</span><span style="color:#e6db74">}
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><ul>
<li>hosts 列表包含<strong>所有</strong> kube-scheduler 节点 IP；</li>
<li>CN 和 O 均为 <code>system:kube-scheduler</code>，kubernetes 内置的 ClusterRoleBindings <code>system:kube-scheduler</code> 将赋予 kube-scheduler 工作所需的权限；</li>
</ul>
<p>生成证书和私钥：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
cfssl gencert -ca<span style="color:#f92672">=</span>/opt/k8s/work/ca.pem <span style="color:#ae81ff">\</span>
  -ca-key<span style="color:#f92672">=</span>/opt/k8s/work/ca-key.pem <span style="color:#ae81ff">\</span>
  -config<span style="color:#f92672">=</span>/opt/k8s/work/ca-config.json <span style="color:#ae81ff">\</span>
  -profile<span style="color:#f92672">=</span>kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
ls kube-scheduler*pem
</code></pre></div><p>将生成的证书和私钥分发到所有 master 节点：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    scp kube-scheduler*.pem root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>:/etc/kubernetes/cert/
  <span style="color:#66d9ef">done</span>
</code></pre></div><h2 id="创建和分发-kubeconfig-文件-1">创建和分发 kubeconfig 文件</h2>
<p>kube-scheduler 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-scheduler 证书：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes <span style="color:#ae81ff">\</span>
  --certificate-authority<span style="color:#f92672">=</span>/opt/k8s/work/ca.pem <span style="color:#ae81ff">\</span>
  --embed-certs<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\</span>
  --server<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://##NODE_IP##:6443&#34;</span> <span style="color:#ae81ff">\</span>
  --kubeconfig<span style="color:#f92672">=</span>kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler <span style="color:#ae81ff">\</span>
  --client-certificate<span style="color:#f92672">=</span>kube-scheduler.pem <span style="color:#ae81ff">\</span>
  --client-key<span style="color:#f92672">=</span>kube-scheduler-key.pem <span style="color:#ae81ff">\</span>
  --embed-certs<span style="color:#f92672">=</span>true <span style="color:#ae81ff">\</span>
  --kubeconfig<span style="color:#f92672">=</span>kube-scheduler.kubeconfig

kubectl config set-context system:kube-scheduler <span style="color:#ae81ff">\</span>
  --cluster<span style="color:#f92672">=</span>kubernetes <span style="color:#ae81ff">\</span>
  --user<span style="color:#f92672">=</span>system:kube-scheduler <span style="color:#ae81ff">\</span>
  --kubeconfig<span style="color:#f92672">=</span>kube-scheduler.kubeconfig

kubectl config use-context system:kube-scheduler --kubeconfig<span style="color:#f92672">=</span>kube-scheduler.kubeconfig
</code></pre></div><p>分发 kubeconfig 到所有 master 节点：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    sed -e <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">s/##NODE_IP##/</span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">&#34;</span> kube-scheduler.kubeconfig &gt; kube-scheduler-<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>.kubeconfig
    scp kube-scheduler-<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>.kubeconfig root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>:/etc/kubernetes/kube-scheduler.kubeconfig
  <span style="color:#66d9ef">done</span>
</code></pre></div><h2 id="创建-kube-scheduler-配置文件">创建 kube-scheduler 配置文件</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
cat &gt;kube-scheduler.yaml.template <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">apiVersion: kubescheduler.config.k8s.io/v1alpha1
</span><span style="color:#e6db74">kind: KubeSchedulerConfiguration
</span><span style="color:#e6db74">bindTimeoutSeconds: 600
</span><span style="color:#e6db74">clientConnection:
</span><span style="color:#e6db74">  burst: 200
</span><span style="color:#e6db74">  kubeconfig: &#34;/etc/kubernetes/kube-scheduler.kubeconfig&#34;
</span><span style="color:#e6db74">  qps: 100
</span><span style="color:#e6db74">enableContentionProfiling: false
</span><span style="color:#e6db74">enableProfiling: true
</span><span style="color:#e6db74">hardPodAffinitySymmetricWeight: 1
</span><span style="color:#e6db74">healthzBindAddress: ##NODE_IP##:10251
</span><span style="color:#e6db74">leaderElection:
</span><span style="color:#e6db74">  leaderElect: true
</span><span style="color:#e6db74">metricsBindAddress: ##NODE_IP##:10251
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><ul>
<li><code>--kubeconfig</code>：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；</li>
<li><code>--leader-elect=true</code>：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li>
</ul>
<p>替换模板文件中的变量：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> <span style="color:#f92672">(</span><span style="color:#f92672">(</span> i<span style="color:#f92672">=</span>0; i &lt; 3; i++ <span style="color:#f92672">)</span><span style="color:#f92672">)</span>
  <span style="color:#66d9ef">do</span>
    sed -e <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">s/##NODE_NAME##/</span><span style="color:#e6db74">${</span>NODE_NAMES[i]<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">&#34;</span> -e <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">s/##NODE_IP##/</span><span style="color:#e6db74">${</span>NODE_IPS[i]<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">&#34;</span> kube-scheduler.yaml.template &gt; kube-scheduler-<span style="color:#e6db74">${</span>NODE_IPS[i]<span style="color:#e6db74">}</span>.yaml
  <span style="color:#66d9ef">done</span>
ls kube-scheduler*.yaml
</code></pre></div><ul>
<li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li>
</ul>
<p>分发 kube-scheduler 配置文件到所有 master 节点：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    scp kube-scheduler-<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>.yaml root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>:/etc/kubernetes/kube-scheduler.yaml
  <span style="color:#66d9ef">done</span>
</code></pre></div><ul>
<li>重命名为 kube-scheduler.yaml;</li>
</ul>
<h2 id="创建-kube-scheduler-systemd-unit-模板文件">创建 kube-scheduler systemd unit 模板文件</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; kube-scheduler.service.template <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">[Unit]
</span><span style="color:#e6db74">Description=Kubernetes Scheduler
</span><span style="color:#e6db74">Documentation=https://github.com/GoogleCloudPlatform/kubernetes
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">[Service]
</span><span style="color:#e6db74">WorkingDirectory=${K8S_DIR}/kube-scheduler
</span><span style="color:#e6db74">ExecStart=/opt/k8s/bin/kube-scheduler \\
</span><span style="color:#e6db74">  --config=/etc/kubernetes/kube-scheduler.yaml \\
</span><span style="color:#e6db74">  --bind-address=##NODE_IP## \\
</span><span style="color:#e6db74">  --secure-port=10259 \\
</span><span style="color:#e6db74">  --port=0 \\
</span><span style="color:#e6db74">  --tls-cert-file=/etc/kubernetes/cert/kube-scheduler.pem \\
</span><span style="color:#e6db74">  --tls-private-key-file=/etc/kubernetes/cert/kube-scheduler-key.pem \\
</span><span style="color:#e6db74">  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
</span><span style="color:#e6db74">  --client-ca-file=/etc/kubernetes/cert/ca.pem \\
</span><span style="color:#e6db74">  --requestheader-allowed-names=&#34;&#34; \\
</span><span style="color:#e6db74">  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\
</span><span style="color:#e6db74">  --requestheader-extra-headers-prefix=&#34;X-Remote-Extra-&#34; \\
</span><span style="color:#e6db74">  --requestheader-group-headers=X-Remote-Group \\
</span><span style="color:#e6db74">  --requestheader-username-headers=X-Remote-User \\
</span><span style="color:#e6db74">  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\
</span><span style="color:#e6db74">  --logtostderr=true \\
</span><span style="color:#e6db74">  --v=2
</span><span style="color:#e6db74">Restart=always
</span><span style="color:#e6db74">RestartSec=5
</span><span style="color:#e6db74">StartLimitInterval=0
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">[Install]
</span><span style="color:#e6db74">WantedBy=multi-user.target
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><h2 id="为各节点创建和分发-kube-scheduler-systemd-unit-文件">为各节点创建和分发 kube-scheduler systemd unit 文件</h2>
<p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> <span style="color:#f92672">(</span><span style="color:#f92672">(</span> i<span style="color:#f92672">=</span>0; i &lt; 3; i++ <span style="color:#f92672">)</span><span style="color:#f92672">)</span>
  <span style="color:#66d9ef">do</span>
    sed -e <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">s/##NODE_NAME##/</span><span style="color:#e6db74">${</span>NODE_NAMES[i]<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">&#34;</span> -e <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">s/##NODE_IP##/</span><span style="color:#e6db74">${</span>NODE_IPS[i]<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">&#34;</span> kube-scheduler.service.template &gt; kube-scheduler-<span style="color:#e6db74">${</span>NODE_IPS[i]<span style="color:#e6db74">}</span>.service 
  <span style="color:#66d9ef">done</span>
ls kube-scheduler*.service
</code></pre></div><p>分发 systemd unit 文件到所有 master 节点：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    scp kube-scheduler-<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>.service root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>:/etc/systemd/system/kube-scheduler.service
  <span style="color:#66d9ef">done</span>
</code></pre></div><h2 id="启动-kube-scheduler-服务">启动 kube-scheduler 服务</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">mkdir -p </span><span style="color:#e6db74">${</span>K8S_DIR<span style="color:#e6db74">}</span><span style="color:#e6db74">/kube-scheduler</span><span style="color:#e6db74">&#34;</span>
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;systemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl restart kube-scheduler&#34;</span>
  <span style="color:#66d9ef">done</span>
</code></pre></div><h2 id="检查服务运行状态-1">检查服务运行状态</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;systemctl status kube-scheduler|grep Active&#34;</span>
  <span style="color:#66d9ef">done</span>
</code></pre></div><p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">journalctl -u kube-scheduler
</code></pre></div><h1 id="08-部署高可用nginx代理">08 部署高可用nginx代理</h1>
<h2 id="基于-nginx-代理的-kube-apiserver-高可用方案">基于 nginx 代理的 kube-apiserver 高可用方案</h2>
<ul>
<li>控制节点的 kube-controller-manager、kube-scheduler 是多实例部署且连接本机的 kube-apiserver，所以只要有一个实例正常，就可以保证高可用；</li>
<li>集群内的 Pod 使用 K8S 服务域名 kubernetes 访问 kube-apiserver， kube-dns 会自动解析出多个 kube-apiserver 节点的 IP，所以也是高可用的；</li>
<li>在每个节点起一个 nginx 进程，后端对接多个 apiserver 实例，nginx 对它们做健康检查和负载均衡；</li>
<li>kubelet、kube-proxy 通过本地的 nginx（监听 127.0.0.1）访问 kube-apiserver，从而实现 kube-apiserver 的高可用；</li>
</ul>
<h2 id="下载和编译-nginx">下载和编译 nginx</h2>
<p>下载源码：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
wget http://nginx.org/download/nginx-1.15.3.tar.gz
tar -xzvf nginx-1.15.3.tar.gz
</code></pre></div><p>配置编译参数：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work/nginx-1.15.3
mkdir nginx-prefix
yum install -y gcc make
./configure --with-stream --without-http --prefix<span style="color:#f92672">=</span><span style="color:#66d9ef">$(</span>pwd<span style="color:#66d9ef">)</span>/nginx-prefix --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module
</code></pre></div><ul>
<li><code>--with-stream</code>：开启 4 层透明转发(TCP Proxy)功能；</li>
<li><code>--without-xxx</code>：关闭所有其他功能，这样生成的动态链接二进制程序依赖最小；</li>
</ul>
<p>输出：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">Configuration summary
  + PCRE library is not used
  + OpenSSL library is not used
  + zlib library is not used

  nginx path prefix: <span style="color:#e6db74">&#34;/root/tmp/nginx-1.15.3/nginx-prefix&#34;</span>
  nginx binary file: <span style="color:#e6db74">&#34;/root/tmp/nginx-1.15.3/nginx-prefix/sbin/nginx&#34;</span>
  nginx modules path: <span style="color:#e6db74">&#34;/root/tmp/nginx-1.15.3/nginx-prefix/modules&#34;</span>
  nginx configuration prefix: <span style="color:#e6db74">&#34;/root/tmp/nginx-1.15.3/nginx-prefix/conf&#34;</span>
  nginx configuration file: <span style="color:#e6db74">&#34;/root/tmp/nginx-1.15.3/nginx-prefix/conf/nginx.conf&#34;</span>
  nginx pid file: <span style="color:#e6db74">&#34;/root/tmp/nginx-1.15.3/nginx-prefix/logs/nginx.pid&#34;</span>
  nginx error log file: <span style="color:#e6db74">&#34;/root/tmp/nginx-1.15.3/nginx-prefix/logs/error.log&#34;</span>
  nginx http access log file: <span style="color:#e6db74">&#34;/root/tmp/nginx-1.15.3/nginx-prefix/logs/access.log&#34;</span>
  nginx http client request body temporary files: <span style="color:#e6db74">&#34;client_body_temp&#34;</span>
  nginx http proxy temporary files: <span style="color:#e6db74">&#34;proxy_temp&#34;</span>
</code></pre></div><p>编译和安装：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work/nginx-1.15.3
make <span style="color:#f92672">&amp;&amp;</span> make install
</code></pre></div><h2 id="验证编译的-nginx">验证编译的 nginx</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work/nginx-1.15.3
./nginx-prefix/sbin/nginx -v
</code></pre></div><p>输出：</p>
<pre><code>nginx version: nginx/1.15.3
</code></pre><h2 id="安装和部署-nginx">安装和部署 nginx</h2>
<p>创建目录结构：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;mkdir -p /opt/k8s/kube-nginx/{conf,logs,sbin}&#34;</span>
  <span style="color:#66d9ef">done</span>
</code></pre></div><p>拷贝二进制程序：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;mkdir -p /opt/k8s/kube-nginx/{conf,logs,sbin}&#34;</span>
    scp /opt/k8s/work/nginx-1.15.3/nginx-prefix/sbin/nginx  root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>:/opt/k8s/kube-nginx/sbin/kube-nginx
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;chmod a+x /opt/k8s/kube-nginx/sbin/*&#34;</span>
  <span style="color:#66d9ef">done</span>
</code></pre></div><ul>
<li>重命名二进制文件为 kube-nginx；</li>
</ul>
<p>配置 nginx，开启 4 层透明转发功能：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
cat &gt; kube-nginx.conf <span style="color:#e6db74">&lt;&lt; \EOF
</span><span style="color:#e6db74">worker_processes 1;
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">events {
</span><span style="color:#e6db74">    worker_connections  1024;
</span><span style="color:#e6db74">}
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">stream {
</span><span style="color:#e6db74">    upstream backend {
</span><span style="color:#e6db74">        hash $remote_addr consistent;
</span><span style="color:#e6db74">        server 10.20.250.23:6443        max_fails=3 fail_timeout=30s;
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    server {
</span><span style="color:#e6db74">        listen *:8443;
</span><span style="color:#e6db74">        proxy_connect_timeout 1s;
</span><span style="color:#e6db74">        proxy_pass backend;
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">}
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><ul>
<li><code>upstream backend</code> 中的 server 列表为集群中各 kube-apiserver 的节点 IP，<strong>需要根据实际情况修改</strong>；</li>
</ul>
<p>分发配置文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-nginx.conf  root@${node_ip}:/opt/k8s/kube-nginx/conf/kube-nginx.conf
  done
</code></pre><h2 id="配置-systemd-unit-文件启动服务">配置 systemd unit 文件，启动服务</h2>
<p>配置 kube-nginx systemd unit 文件：</p>
<pre><code>cd /opt/k8s/work
cat &gt; kube-nginx.service &lt;&lt;EOF
[Unit]
Description=kube-apiserver nginx proxy
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=forking
ExecStartPre=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -t
ExecStart=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx
ExecReload=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -s reload
PrivateTmp=true
Restart=always
RestartSec=5
StartLimitInterval=0
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre><p>分发 systemd unit 文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp kube-nginx.service  root@${node_ip}:/etc/systemd/system/
  done
</code></pre><p>启动 kube-nginx 服务：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-nginx &amp;&amp; systemctl restart kube-nginx&quot;
  done
</code></pre><h2 id="检查-kube-nginx-服务运行状态">检查 kube-nginx 服务运行状态</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl status kube-nginx |grep 'Active:'&quot;
  done
</code></pre><p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u kube-nginx
</code></pre><h1 id="09-部署-containerd-组件或者部署docker">09 部署 containerd 组件或者部署docker</h1>
<p>containerd 实现了 kubernetes 的 Container Runtime Interface (CRI) 接口，提供容器运行时核心功能，如镜像管理、容器管理等，相比 dockerd 更加简单、健壮和可移植。</p>
<h2 id="创建和分发-containerd-配置文件">创建和分发 containerd 配置文件</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat <span style="color:#e6db74">&lt;&lt; EOF | sudo tee containerd-config.toml
</span><span style="color:#e6db74">version = 2
</span><span style="color:#e6db74">root = &#34;${CONTAINERD_DIR}/root&#34;
</span><span style="color:#e6db74">state = &#34;${CONTAINERD_DIR}/state&#34;
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">[plugins]
</span><span style="color:#e6db74">  [plugins.&#34;io.containerd.grpc.v1.cri&#34;]
</span><span style="color:#e6db74">    sandbox_image = &#34;registry.cn-beijing.aliyuncs.com/images_k8s/pause-amd64:3.1&#34;
</span><span style="color:#e6db74">    [plugins.&#34;io.containerd.grpc.v1.cri&#34;.cni]
</span><span style="color:#e6db74">      bin_dir = &#34;/opt/k8s/bin&#34;
</span><span style="color:#e6db74">      conf_dir = &#34;/etc/cni/net.d&#34;
</span><span style="color:#e6db74">  [plugins.&#34;io.containerd.runtime.v1.linux&#34;]
</span><span style="color:#e6db74">    shim = &#34;containerd-shim&#34;
</span><span style="color:#e6db74">    runtime = &#34;runc&#34;
</span><span style="color:#e6db74">    runtime_root = &#34;&#34;
</span><span style="color:#e6db74">    no_shim = false
</span><span style="color:#e6db74">    shim_debug = false
</span><span style="color:#e6db74">EOF</span>
cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">mkdir -p /etc/containerd/ </span><span style="color:#e6db74">${</span>CONTAINERD_DIR<span style="color:#e6db74">}</span><span style="color:#e6db74">/{root,state}</span><span style="color:#e6db74">&#34;</span>
    scp containerd-config.toml root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>:/etc/containerd/config.toml
  <span style="color:#66d9ef">done</span>
</code></pre></div><h2 id="创建-containerd-systemd-unit-文件">创建 containerd systemd unit 文件</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
cat <span style="color:#e6db74">&lt;&lt;EOF | sudo tee containerd.service
</span><span style="color:#e6db74">[Unit]
</span><span style="color:#e6db74">Description=containerd container runtime
</span><span style="color:#e6db74">Documentation=https://containerd.io
</span><span style="color:#e6db74">After=network.target
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">[Service]
</span><span style="color:#e6db74">Environment=&#34;PATH=/opt/k8s/bin:/bin:/sbin:/usr/bin:/usr/sbin&#34;
</span><span style="color:#e6db74">ExecStartPre=/sbin/modprobe overlay
</span><span style="color:#e6db74">ExecStart=/opt/k8s/bin/containerd
</span><span style="color:#e6db74">Restart=always
</span><span style="color:#e6db74">RestartSec=5
</span><span style="color:#e6db74">Delegate=yes
</span><span style="color:#e6db74">KillMode=process
</span><span style="color:#e6db74">OOMScoreAdjust=-999
</span><span style="color:#e6db74">LimitNOFILE=1048576
</span><span style="color:#e6db74">LimitNPROC=infinity
</span><span style="color:#e6db74">LimitCORE=infinity
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">[Install]
</span><span style="color:#e6db74">WantedBy=multi-user.target
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><h2 id="分发-systemd-unit-文件启动-containerd-服务">分发 systemd unit 文件，启动 containerd 服务</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    scp containerd.service root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>:/etc/systemd/system
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;systemctl enable containerd &amp;&amp; systemctl restart containerd&#34;</span>
  <span style="color:#66d9ef">done</span>
</code></pre></div><h2 id="创建和分发-crictl-配置文件">创建和分发 crictl 配置文件</h2>
<p>crictl 是兼容 CRI 容器运行时的命令行工具，提供类似于 docker 命令的功能。具体参考<a href="https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md">官方文档</a>。</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
cat <span style="color:#e6db74">&lt;&lt; EOF | sudo tee crictl.yaml
</span><span style="color:#e6db74">runtime-endpoint: unix:///run/containerd/containerd.sock
</span><span style="color:#e6db74">image-endpoint: unix:///run/containerd/containerd.sock
</span><span style="color:#e6db74">timeout: 10
</span><span style="color:#e6db74">debug: false
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p>分发到所有 worker 节点：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    scp crictl.yaml root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>:/etc/crictl.yaml
  <span style="color:#66d9ef">done</span>
</code></pre></div><h2 id="部署docker可选">部署docker（可选）</h2>
<p>安装完docker后修改一下参数：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cat &gt; docker-daemon.json <span style="color:#e6db74">&lt;&lt;EOF
</span><span style="color:#e6db74">{
</span><span style="color:#e6db74">    &#34;registry-mirrors&#34;: [&#34;https://docker.mirrors.ustc.edu.cn&#34;,&#34;https://hub-mirror.c.163.com&#34;],
</span><span style="color:#e6db74">    &#34;insecure-registries&#34;: [&#34;docker02:35000&#34;],
</span><span style="color:#e6db74">    &#34;max-concurrent-downloads&#34;: 20,
</span><span style="color:#e6db74">    &#34;live-restore&#34;: true,
</span><span style="color:#e6db74">    &#34;max-concurrent-uploads&#34;: 10,
</span><span style="color:#e6db74">    &#34;debug&#34;: true,
</span><span style="color:#e6db74">    &#34;data-root&#34;: &#34;${DOCKER_DIR}/data&#34;,
</span><span style="color:#e6db74">    &#34;exec-root&#34;: &#34;${DOCKER_DIR}/exec&#34;,
</span><span style="color:#e6db74">    &#34;log-opts&#34;: {
</span><span style="color:#e6db74">      &#34;max-size&#34;: &#34;100m&#34;,
</span><span style="color:#e6db74">      &#34;max-file&#34;: &#34;5&#34;
</span><span style="color:#e6db74">    }
</span><span style="color:#e6db74">}
</span><span style="color:#e6db74">EOF</span>
</code></pre></div><p>分发 docker 配置文件到所有 worker 节点：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    ssh root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">mkdir -p  /etc/docker/ </span><span style="color:#e6db74">${</span>DOCKER_DIR<span style="color:#e6db74">}</span><span style="color:#e6db74">/{data,exec}</span><span style="color:#e6db74">&#34;</span>
    scp docker-daemon.json root@<span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span>:/etc/docker/daemon.json
  <span style="color:#66d9ef">done</span>
</code></pre></div><h2 id="更新-kubelet-配置并重启服务每个节点上都操作">更新 kubelet 配置并重启服务（每个节点上都操作）</h2>
<p>需要删除 kubelet 的 systemd unit 文件(/etc/systemd/system/kubelet.service)，删除下面 4 行：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">  --network-plugin<span style="color:#f92672">=</span>cni <span style="color:#ae81ff">\\</span>
  --cni-conf-dir<span style="color:#f92672">=</span>/etc/cni/net.d <span style="color:#ae81ff">\\</span>
  --container-runtime<span style="color:#f92672">=</span>remote <span style="color:#ae81ff">\\</span>
  --container-runtime-endpoint<span style="color:#f92672">=</span>unix:///var/run/containerd/containerd.sock <span style="color:#ae81ff">\\</span>
</code></pre></div><p>然后重启 kubelet 服务：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">systemctl restart kubelet
</code></pre></div><h1 id="10-部署-kubelet-组件">10 部署 kubelet 组件</h1>
<p>kubelet 运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如 exec、run、logs 等。kubelet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况。为确保安全，部署时关闭了 kubelet 的非安全 http 端口，对请求进行认证和授权，拒绝未授权的访问(如 apiserver、heapster 的请求)。</p>
<h2 id="创建-kubelet-bootstrap-kubeconfig-文件">创建 kubelet bootstrap kubeconfig 文件</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;

    # 创建 token
    export BOOTSTRAP_TOKEN=$(kubeadm token create \
      --description kubelet-bootstrap-token \
      --groups system:bootstrappers:${node_name} \
      --kubeconfig ~/.kube/config)

    # 设置集群参数
    kubectl config set-cluster kubernetes \
      --certificate-authority=/etc/kubernetes/cert/ca.pem \
      --embed-certs=true \
      --server=${KUBE_APISERVER} \
      --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig

    # 设置客户端认证参数
    kubectl config set-credentials kubelet-bootstrap \
      --token=${BOOTSTRAP_TOKEN} \
      --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig

    # 设置上下文参数
    kubectl config set-context default \
      --cluster=kubernetes \
      --user=kubelet-bootstrap \
      --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig

    # 设置默认上下文
    kubectl config use-context default --kubeconfig=kubelet-bootstrap-${node_name}.kubeconfig
  done
</code></pre><ul>
<li>向 kubeconfig 写入的是 token，bootstrap 结束后 kube-controller-manager 为 kubelet 创建 client 和 server 证书；</li>
</ul>
<p>查看 kubeadm 为各节点创建的 token：</p>
<pre><code>$ kubeadm token list --kubeconfig ~/.kube/config
TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION               EXTRA GROUPS
2sb8wy.euialqfpxfbcljby   23h       2020-02-08T15:36:30+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:zhangjun-k8s-02
ta7onm.fcen74h0mczyfbz2   23h       2020-02-08T15:36:30+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:zhangjun-k8s-01
xk27zp.tylnvywx9kc8sq87   23h       2020-02-08T15:36:30+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:zhangjun-k8s-03
</code></pre><ul>
<li>token 有效期为 1 天，<strong>超期后将不能</strong>再被用来 boostrap kubelet，且会被 kube-controller-manager 的 tokencleaner 清理；</li>
<li>kube-apiserver 接收 kubelet 的 bootstrap token 后，将请求的 user 设置为 <code>system:bootstrap:&lt;Token ID&gt;</code>，group 设置为 <code>system:bootstrappers</code>，后续将为这个 group 设置 ClusterRoleBinding；</li>
</ul>
<h2 id="分发-bootstrap-kubeconfig-文件到所有-worker-节点">分发 bootstrap kubeconfig 文件到所有 worker 节点</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    scp kubelet-bootstrap-${node_name}.kubeconfig root@${node_name}:/etc/kubernetes/kubelet-bootstrap.kubeconfig
  done
</code></pre><h2 id="创建和分发-kubelet-参数配置文件">创建和分发 kubelet 参数配置文件</h2>
<p>从 v1.10 开始，部分 kubelet 参数需在<strong>配置文件</strong>中配置，<code>kubelet --help</code> 会提示：</p>
<pre><code>DEPRECATED: This parameter should be set via the config file specified by the Kubelet's --config flag
</code></pre><p>创建 kubelet 参数配置文件模板（可配置项参考<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/config/types.go">代码中注释</a>）：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; kubelet-config.yaml.template &lt;&lt;EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: &quot;##NODE_IP##&quot;
staticPodPath: &quot;&quot;
syncFrequency: 1m
fileCheckFrequency: 20s
httpCheckFrequency: 20s
staticPodURL: &quot;&quot;
port: 10250
readOnlyPort: 0
rotateCertificates: true
serverTLSBootstrap: true
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: &quot;/etc/kubernetes/cert/ca.pem&quot;
authorization:
  mode: Webhook
registryPullQPS: 0
registryBurst: 20
eventRecordQPS: 0
eventBurst: 20
enableDebuggingHandlers: true
enableContentionProfiling: true
healthzPort: 10248
healthzBindAddress: &quot;##NODE_IP##&quot;
clusterDomain: &quot;${CLUSTER_DNS_DOMAIN}&quot;
clusterDNS:
  - &quot;${CLUSTER_DNS_SVC_IP}&quot;
nodeStatusUpdateFrequency: 10s
nodeStatusReportFrequency: 1m
imageMinimumGCAge: 2m
imageGCHighThresholdPercent: 85
imageGCLowThresholdPercent: 80
volumeStatsAggPeriod: 1m
kubeletCgroups: &quot;&quot;
systemCgroups: &quot;&quot;
cgroupRoot: &quot;&quot;
cgroupsPerQOS: true
cgroupDriver: cgroupfs
runtimeRequestTimeout: 10m
hairpinMode: promiscuous-bridge
maxPods: 220
podCIDR: &quot;${CLUSTER_CIDR}&quot;
podPidsLimit: -1
resolvConf: /etc/resolv.conf
maxOpenFiles: 1000000
kubeAPIQPS: 1000
kubeAPIBurst: 2000
serializeImagePulls: false
evictionHard:
  memory.available:  &quot;100Mi&quot;
  nodefs.available:  &quot;10%&quot;
  nodefs.inodesFree: &quot;5%&quot;
  imagefs.available: &quot;15%&quot;
evictionSoft: {}
enableControllerAttachDetach: true
failSwapOn: true
containerLogMaxSize: 20Mi
containerLogMaxFiles: 10
systemReserved: {}
kubeReserved: {}
systemReservedCgroup: &quot;&quot;
kubeReservedCgroup: &quot;&quot;
enforceNodeAllocatable: [&quot;pods&quot;]
EOF
</code></pre><ul>
<li>address：kubelet 安全端口（https，10250）监听的地址，不能为 127.0.0.1，否则 kube-apiserver、heapster 等不能调用 kubelet 的 API；</li>
<li>readOnlyPort=0：关闭只读端口(默认 10255)，等效为未指定；</li>
<li>authentication.anonymous.enabled：设置为 false，不允许匿名�访问 10250 端口；</li>
<li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTP 证书认证；</li>
<li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li>
<li>对于未通过 x509 证书和 webhook 认证的请求(kube-apiserver 或其他客户端)，将被拒绝，提示 Unauthorized；</li>
<li>authroization.mode=Webhook：kubelet 使用 SubjectAccessReview API 查询 kube-apiserver 某 user、group 是否具有操作资源的权限(RBAC)；</li>
<li>featureGates.RotateKubeletClientCertificate、featureGates.RotateKubeletServerCertificate：自动 rotate 证书，证书的有效期取决于 kube-controller-manager 的 &ndash;experimental-cluster-signing-duration 参数；</li>
<li>需要 root 账户运行；</li>
</ul>
<p>为各节点创建和分发 kubelet 配置文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do 
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    sed -e &quot;s/##NODE_IP##/${node_ip}/&quot; kubelet-config.yaml.template &gt; kubelet-config-${node_ip}.yaml.template
    scp kubelet-config-${node_ip}.yaml.template root@${node_ip}:/etc/kubernetes/kubelet-config.yaml
  done
</code></pre><h2 id="创建和分发-kubelet-systemd-unit-文件">创建和分发 kubelet systemd unit 文件</h2>
<p>创建 kubelet systemd unit 文件模板：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; kubelet.service.template &lt;&lt;EOF
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
WorkingDirectory=${K8S_DIR}/kubelet
ExecStart=/opt/k8s/bin/kubelet \\
  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \\
  --cert-dir=/etc/kubernetes/cert \\
  --network-plugin=cni \\
  --cni-conf-dir=/etc/cni/net.d \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --root-dir=${K8S_DIR}/kubelet \\
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\
  --config=/etc/kubernetes/kubelet-config.yaml \\
  --hostname-override=##NODE_NAME## \\
  --image-pull-progress-deadline=15m \\
  --volume-plugin-dir=${K8S_DIR}/kubelet/kubelet-plugins/volume/exec/ \\
  --logtostderr=true \\
  --v=2
Restart=always
RestartSec=5
StartLimitInterval=0

[Install]
WantedBy=multi-user.target
EOF
</code></pre><ul>
<li>如果设置了 <code>--hostname-override</code> 选项，则 <code>kube-proxy</code> 也需要设置该选项，否则会出现找不到 Node 的情况；</li>
<li><code>--bootstrap-kubeconfig</code>：指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；</li>
<li>K8S approve kubelet 的 csr 请求后，在 <code>--cert-dir</code> 目录创建证书和私钥文件，然后写入 <code>--kubeconfig</code> 文件；</li>
<li><code>--pod-infra-container-image</code> 不使用 redhat 的 <code>pod-infrastructure:latest</code> 镜像，它不能回收容器的僵尸；</li>
</ul>
<p>为各节点创建和分发 kubelet systemd unit 文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do 
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    sed -e &quot;s/##NODE_NAME##/${node_name}/&quot; kubelet.service.template &gt; kubelet-${node_name}.service
    scp kubelet-${node_name}.service root@${node_name}:/etc/systemd/system/kubelet.service
  done
</code></pre><h2 id="授予-kube-apiserver-访问-kubelet-api-的权限">授予 kube-apiserver 访问 kubelet API 的权限</h2>
<p>在执行 kubectl exec、run、logs 等命令时，apiserver 会将请求转发到 kubelet 的 https 端口。这里定义 RBAC 规则，授权 apiserver 使用的证书（kubernetes.pem）用户名（CN：kuberntes-master）访问 kubelet API 的权限：</p>
<pre><code>kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes-master
</code></pre><h2 id="bootstrap-token-auth-和授予权限">Bootstrap Token Auth 和授予权限</h2>
<p>kubelet 启动时查找 <code>--kubeletconfig</code> 参数对应的文件是否存在，如果不存在则使用 <code>--bootstrap-kubeconfig</code> 指定的 kubeconfig 文件向 kube-apiserver 发送证书签名请求 (CSR)。</p>
<p>kube-apiserver 收到 CSR 请求后，对其中的 Token 进行认证，认证通过后将请求的 user 设置为 <code>system:bootstrap:&lt;Token ID&gt;</code>，group 设置为 <code>system:bootstrappers</code>，这一过程称为 <code>Bootstrap Token Auth</code>。</p>
<p>默认情况下，这个 user 和 group 没有创建 CSR 的权限，kubelet 启动失败，错误日志如下：</p>
<pre><code>$ sudo journalctl -u kubelet -a |grep -A 2 'certificatesigningrequests'
May 26 12:13:41 zhangjun-k8s-01 kubelet[128468]: I0526 12:13:41.798230  128468 certificate_manager.go:366] Rotating certificates
May 26 12:13:41 zhangjun-k8s-01 kubelet[128468]: E0526 12:13:41.801997  128468 certificate_manager.go:385] Failed while requesting a signed certificate from the master: cannot create certificate signing request: certificatesigningrequests.certificates.k8s.io is forbidden: User &quot;system:bootstrap:82jfrm&quot; cannot create resource &quot;certificatesigningrequests&quot; in API group &quot;certificates.k8s.io&quot; at the cluster scope
</code></pre><p>解决办法是：创建一个 clusterrolebinding，将 group system:bootstrappers 和 clusterrole system:node-bootstrapper 绑定：</p>
<pre><code>kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers
</code></pre><h2 id="自动-approve-csr-请求生成-kubelet-client-证书">自动 approve CSR 请求，生成 kubelet client 证书</h2>
<p>kubelet 创建 CSR 请求后，下一步需要创建被 approve，有两种方式：</p>
<ol>
<li>kube-controller-manager 自动 aprrove；</li>
<li>手动使用命令 <code>kubectl certificate approve</code>；</li>
</ol>
<p>CSR 被 approve 后，kubelet 向 kube-controller-manager 请求创建 client 证书，kube-controller-manager 中的 <code>csrapproving</code> controller 使用 <code>SubjectAccessReview</code> API 来检查 kubelet 请求（对应的 group 是 system:bootstrappers）是否具有相应的权限。</p>
<p>创建三个 ClusterRoleBinding，分别授予 group system:bootstrappers 和 group system:nodes 进行 approve client、renew client、renew server 证书的权限（server csr 是手动 approve 的，见后文）：</p>
<pre><code>cd /opt/k8s/work
cat &gt; csr-crb.yaml &lt;&lt;EOF
 # Approve all CSRs for the group &quot;system:bootstrappers&quot;
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: auto-approve-csrs-for-group
 subjects:
 - kind: Group
   name: system:bootstrappers
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
   apiGroup: rbac.authorization.k8s.io
---
 # To let a node of the group &quot;system:nodes&quot; renew its own credentials
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: node-client-cert-renewal
 subjects:
 - kind: Group
   name: system:nodes
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
   apiGroup: rbac.authorization.k8s.io
---
# A ClusterRole which instructs the CSR approver to approve a node requesting a
# serving cert matching its client cert.
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: approve-node-server-renewal-csr
rules:
- apiGroups: [&quot;certificates.k8s.io&quot;]
  resources: [&quot;certificatesigningrequests/selfnodeserver&quot;]
  verbs: [&quot;create&quot;]
---
 # To let a node of the group &quot;system:nodes&quot; renew its own server credentials
 kind: ClusterRoleBinding
 apiVersion: rbac.authorization.k8s.io/v1
 metadata:
   name: node-server-cert-renewal
 subjects:
 - kind: Group
   name: system:nodes
   apiGroup: rbac.authorization.k8s.io
 roleRef:
   kind: ClusterRole
   name: approve-node-server-renewal-csr
   apiGroup: rbac.authorization.k8s.io
EOF
kubectl apply -f csr-crb.yaml
</code></pre><ul>
<li>auto-approve-csrs-for-group：自动 approve node 的第一次 CSR； 注意第一次 CSR 时，请求的 Group 为 system:bootstrappers；</li>
<li>node-client-cert-renewal：自动 approve node 后续过期的 client 证书，自动生成的证书 Group 为 system:nodes;</li>
<li>node-server-cert-renewal：自动 approve node 后续过期的 server 证书，自动生成的证书 Group 为 system:nodes;</li>
</ul>
<h2 id="启动-kubelet-服务">启动 kubelet 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p ${K8S_DIR}/kubelet/kubelet-plugins/volume/exec/&quot;
    ssh root@${node_ip} &quot;/usr/sbin/swapoff -a&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet&quot;
  done
</code></pre><ul>
<li>启动服务前必须先创建工作目录；</li>
<li>关闭 swap 分区，否则 kubelet 会启动失败；</li>
</ul>
<p>kubelet 启动后使用 &ndash;bootstrap-kubeconfig 向 kube-apiserver 发送 CSR 请求，当这个 CSR 被 approve 后，kube-controller-manager 为 kubelet 创建 TLS 客户端证书、私钥和 &ndash;kubeletconfig 文件。</p>
<p>注意：kube-controller-manager 需要配置 <code>--cluster-signing-cert-file</code> 和 <code>--cluster-signing-key-file</code> 参数，才会为 TLS Bootstrap 创建证书和私钥。</p>
<h2 id="手动-approve-server-cert-csr">手动 approve server cert csr</h2>
<p>基于<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#kubelet-configuration">安全性考虑</a>，CSR approving controllers 不会自动 approve kubelet server 证书签名请求，需要手动 approve：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ kubectl get csr
NAME        AGE     REQUESTOR                     CONDITION
csr-5rwzm   3m22s   system:node:zhangjun-k8s-01   Pending
csr-65nms   3m34s   system:bootstrap:2sb8wy       Approved,Issued
csr-8t5hj   3m21s   system:node:zhangjun-k8s-02   Pending
csr-jkhhs   3m20s   system:node:zhangjun-k8s-03   Pending
csr-jv7dn   3m35s   system:bootstrap:ta7onm       Approved,Issued
csr-vb6p5   3m33s   system:bootstrap:xk27zp       Approved,Issued

$ <span style="color:#75715e"># 手动 approve</span>
$ kubectl get csr | grep Pending | awk <span style="color:#e6db74">&#39;{print $1}&#39;</span> | xargs kubectl certificate approve

$ <span style="color:#75715e"># 自动生成了 server 证书</span>
$  ls -l /etc/kubernetes/cert/kubelet-*
-rw------- <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">1281</span> Feb  <span style="color:#ae81ff">7</span> 15:38 /etc/kubernetes/cert/kubelet-client-2020-02-07-15-38-21.pem
lrwxrwxrwx <span style="color:#ae81ff">1</span> root root   <span style="color:#ae81ff">59</span> Feb  <span style="color:#ae81ff">7</span> 15:38 /etc/kubernetes/cert/kubelet-client-current.pem -&gt; /etc/kubernetes/cert/kubelet-client-2020-02-07-15-38-21.pem
-rw------- <span style="color:#ae81ff">1</span> root root <span style="color:#ae81ff">1330</span> Feb  <span style="color:#ae81ff">7</span> 15:42 /etc/kubernetes/cert/kubelet-server-2020-02-07-15-42-12.pem
lrwxrwxrwx <span style="color:#ae81ff">1</span> root root   <span style="color:#ae81ff">59</span> Feb  <span style="color:#ae81ff">7</span> 15:42 /etc/kubernetes/cert/kubelet-server-current.pem -&gt; /etc/kubernetes/cert/kubelet-server-2020-02
</code></pre></div><h3 id="bear-token-认证和授权">bear token 认证和授权</h3>
<p>创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限：</p>
<pre><code>kubectl create sa kubelet-api-test
kubectl create clusterrolebinding kubelet-api-test --clusterrole=system:kubelet-api-admin --serviceaccount=default:kubelet-api-test
SECRET=$(kubectl get secrets | grep kubelet-api-test | awk '{print $1}')
TOKEN=$(kubectl describe secret ${SECRET} | grep -E '^token' | awk '{print $2}')
echo ${TOKEN}
$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer ${TOKEN}&quot; https://172.27.138.251:10250/metrics | head
# HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.
# TYPE apiserver_audit_event_total counter
apiserver_audit_event_total 0
# HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.
# TYPE apiserver_audit_requests_rejected_total counter
apiserver_audit_requests_rejected_total 0
# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.
# TYPE apiserver_client_certificate_expiration_seconds histogram
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;0&quot;} 0
apiserver_client_certificate_expiration_seconds_bucket{le=&quot;1800&quot;} 0
</code></pre><h3 id="cadvisor-和-metrics">cadvisor 和 metrics</h3>
<p>cadvisor 是内嵌在 kubelet 二进制中的，统计所在节点各容器的资源(CPU、内存、磁盘、网卡)使用情况的服务。</p>
<p>浏览器访问 https://ip:10250/metrics 和 https://ip:10250/metrics/cadvisor 分别返回 kubelet 和 cadvisor 的 metrics。</p>
<p>注意：</p>
<ul>
<li>kubelet.config.json 设置 authentication.anonymous.enabled 为 false，不允许匿名证书访问 10250 的 https 服务；</li>
</ul>
<h1 id="11-部署-kube-proxy-组件">11 部署 kube-proxy 组件</h1>
<p>kube-proxy 运行在所有 worker 节点上，它监听 apiserver 中 service 和 endpoint 的变化情况，创建路由规则以提供服务 IP 和负载均衡功能。</p>
<h2 id="创建-kube-proxy-证书">创建 kube-proxy 证书</h2>
<p>创建证书签名请求：</p>
<pre><code>cd /opt/k8s/work
cat &gt; kube-proxy-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;system:kube-proxy&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;opsnull&quot;
    }
  ]
}
EOF
</code></pre><ul>
<li>CN：指定该证书的 User 为 <code>system:kube-proxy</code>；</li>
<li>预定义的 RoleBinding <code>system:node-proxier</code> 将User <code>system:kube-proxy</code> 与 Role <code>system:node-proxier</code> 绑定，该 Role 授予了调用 <code>kube-apiserver</code> Proxy 相关 API 的权限；</li>
<li>该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cd /opt/k8s/work
cfssl gencert -ca=/opt/k8s/work/ca.pem \
  -ca-key=/opt/k8s/work/ca-key.pem \
  -config=/opt/k8s/work/ca-config.json \
  -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
ls kube-proxy*
</code></pre><h2 id="创建和分发-kubeconfig-文件-2">创建和分发 kubeconfig 文件</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
kubectl config set-cluster kubernetes \
  --certificate-authority=/opt/k8s/work/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
</code></pre><p>分发 kubeconfig 文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    scp kube-proxy.kubeconfig root@${node_name}:/etc/kubernetes/
  done
</code></pre><h2 id="创建-kube-proxy-配置文件">创建 kube-proxy 配置文件</h2>
<p>从 v1.10 开始，kube-proxy <strong>部分参数</strong>可以配置文件中配置。可以使用 <code>--write-config-to</code> 选项生成该配置文件，或者参考 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.14/pkg/proxy/apis/config/types.go">源代码的注释</a>。</p>
<p>创建 kube-proxy config 文件模板：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; kube-proxy-config.yaml.template &lt;&lt;EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  burst: 200
  kubeconfig: &quot;/etc/kubernetes/kube-proxy.kubeconfig&quot;
  qps: 100
bindAddress: ##NODE_IP##
healthzBindAddress: ##NODE_IP##:10256
metricsBindAddress: ##NODE_IP##:10249
enableProfiling: true
clusterCIDR: ${CLUSTER_CIDR}
hostnameOverride: ##NODE_NAME##
mode: &quot;ipvs&quot;
portRange: &quot;&quot;
iptables:
  masqueradeAll: false
ipvs:
  scheduler: rr
  excludeCIDRs: []
EOF
</code></pre><ul>
<li><code>bindAddress</code>: 监听地址；</li>
<li><code>clientConnection.kubeconfig</code>: 连接 apiserver 的 kubeconfig 文件；</li>
<li><code>clusterCIDR</code>: kube-proxy 根据 <code>--cluster-cidr</code> 判断集群内部和外部流量，指定 <code>--cluster-cidr</code> 或 <code>--masquerade-all</code> 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；</li>
<li><code>hostnameOverride</code>: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则；</li>
<li><code>mode</code>: 使用 ipvs 模式；</li>
</ul>
<p>为各节点创建和分发 kube-proxy 配置文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for (( i=0; i &lt; ${#NODE_IPS[@]}; i++ ))
  do 
    echo &quot;&gt;&gt;&gt; ${NODE_NAMES[i]}&quot;
    sed -e &quot;s/##NODE_NAME##/${NODE_NAMES[i]}/&quot; -e &quot;s/##NODE_IP##/${NODE_IPS[i]}/&quot; kube-proxy-config.yaml.template &gt; kube-proxy-config-${NODE_NAMES[i]}.yaml.template
    scp kube-proxy-config-${NODE_NAMES[i]}.yaml.template root@${NODE_NAMES[i]}:/etc/kubernetes/kube-proxy-config.yaml
  done
</code></pre><h2 id="创建和分发-kube-proxy-systemd-unit-文件">创建和分发 kube-proxy systemd unit 文件</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; kube-proxy.service &lt;&lt;EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=${K8S_DIR}/kube-proxy
ExecStart=/opt/k8s/bin/kube-proxy \\
  --config=/etc/kubernetes/kube-proxy-config.yaml \\
  --logtostderr=true \\
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
</code></pre><p>分发 kube-proxy systemd unit 文件：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_name in ${NODE_NAMES[@]}
  do 
    echo &quot;&gt;&gt;&gt; ${node_name}&quot;
    scp kube-proxy.service root@${node_name}:/etc/systemd/system/
  done
</code></pre><h2 id="启动-kube-proxy-服务">启动 kube-proxy 服务</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p ${K8S_DIR}/kube-proxy&quot;
    ssh root@${node_ip} &quot;modprobe ip_vs_rr&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy&quot;
  done
</code></pre><h2 id="检查启动结果">检查启动结果</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl status kube-proxy|grep Active&quot;
  done
</code></pre><p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">journalctl -u kube-proxy
</code></pre></div><h1 id="12-部署-calico-网络">12 部署 calico 网络</h1>
<p>kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。calico 使用 IPIP 或 BGP 技术（默认为 IPIP）为各节点创建一个可以互通的 Pod 网络。如果使用 flannel，请参考附件 <a href="https://github.com/opsnull/follow-me-install-kubernetes-cluster/blob/master/E.%E9%83%A8%E7%BD%B2flannel%E7%BD%91%E7%BB%9C.md">E.部署flannel网络.md</a>（flannel 与 docker 结合使用）。</p>
<h2 id="安装-calico-网络插件">安装 calico 网络插件</h2>
<pre><code>cd /opt/k8s/work
curl https://docs.projectcalico.org/manifests/calico.yaml -O
</code></pre><p>修改配置：</p>
<pre><code>$ cp calico.yaml calico.yaml.orig
$ diff calico.yaml.orig calico.yaml
630c630,632
&lt;               value: &quot;192.168.0.0/16&quot;
---
&gt;               value: &quot;172.30.0.0/16&quot;
&gt;             - name: IP_AUTODETECTION_METHOD
&gt;               value: &quot;interface=eth.*&quot;
699c701
&lt;             path: /opt/cni/bin
---
&gt;             path: /opt/k8s/bin
</code></pre><ul>
<li>将 Pod 网段地址修改为 <code>172.30.0.0/16</code>;</li>
<li>calico 自动探查互联网卡，如果有多快网卡，则可以配置用于互联的网络接口命名正则表达式，如上面的 <code>eth.*</code>(根据自己服务器的网络接口名修改)；</li>
</ul>
<p>运行 calico 插件：</p>
<pre><code>$ kubectl apply -f  calico.yaml
</code></pre><ul>
<li>calico 插架以 daemonset 方式运行在所有的 K8S 节点上。</li>
</ul>
<h2 id="查看-calico-运行状态">查看 calico 运行状态</h2>
<pre><code>$ kubectl get pods -n kube-system -o wide
NAME                                      READY   STATUS    RESTARTS   AGE     IP               NODE              NOMINATED NODE   READINESS GATES
calico-kube-controllers-77c4b7448-99lfq   1/1     Running   0          2m11s   172.30.184.128   zhangjun-k8s-03   &lt;none&gt;           &lt;none&gt;
calico-node-dxnjs                         1/1     Running   0          2m11s   172.27.137.229   zhangjun-k8s-02   &lt;none&gt;           &lt;none&gt;
calico-node-rknzz                         1/1     Running   0          2m11s   172.27.138.239   zhangjun-k8s-03   &lt;none&gt;           &lt;none&gt;
calico-node-rw84c                         1/1     Running   0          2m11s   172.27.138.251   zhangjun-k8s-01   &lt;none&gt;           &lt;none&gt;
</code></pre><p>使用 crictl 命令查看 calico 使用的镜像：</p>
<pre><code>$ crictl  images
IMAGE                                                     TAG                 IMAGE ID            SIZE
docker.io/calico/cni                                      v3.12.0             cb6799752c46c       66.5MB
docker.io/calico/node                                     v3.12.0             fc05bc4225f39       89.7MB
docker.io/calico/pod2daemon-flexvol                       v3.12.0             98793d0a88c82       37.5MB
registry.cn-beijing.aliyuncs.com/images_k8s/pause-amd64   3.1                 21a595adc69ca       326kB
</code></pre><ul>
<li>
<p>如果 crictl 输出为空或执行失败，则有可能是缺少配置文件 <code>/etc/crictl.yaml</code> 导致的，该文件的配置如下：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">$ cat /etc/crictl.yaml
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
</code></pre></div></li>
</ul>
<h1 id="13-部署-flannel-网络可选">13 部署 flannel 网络（可选）</h1>
<p>kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472（<strong>需要开放该端口</strong>，如公有云 AWS 等）。</p>
<p>flanneld 第一次启动时，从 etcd 获取配置的 Pod 网段信息，为本节点分配一个未使用的地址段，然后创建 <code>flannedl.1</code> 网络接口（也可能是其它名称，如 flannel1 等）。</p>
<p>flannel 将分配给自己的 Pod 网段信息写入 <code>/run/flannel/docker</code> 文件，docker 后续使用这个文件中的环境变量设置 <code>docker0</code> 网桥，从而从这个地址段为本节点的所有 Pod 容器分配 IP。</p>
<p>注意：</p>
<ol>
<li>如果没有特殊指明，本文档的所有操作<strong>均在 zhangjun-k8s01 节点上执行</strong>，然后远程分发文件和执行命令；</li>
<li>flanneld 与本文档部署的 etcd v3.4.x 不兼容，需要将 etcd 降级到 v3.3.x；</li>
<li>flanneld 与 docker 结合使用；</li>
</ol>
<h2 id="调整etcd-systemd-unit文件">调整etcd systemd unit文件</h2>
<p>etcd3.4.2默认关闭v2的api，需要在启动文件中开启：</p>
<ol>
<li>修改etcd.service systemd unit文件</li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"> ....
  --heartbeat-interval<span style="color:#f92672">=</span><span style="color:#ae81ff">250</span> <span style="color:#ae81ff">\</span>
  --election-timeout<span style="color:#f92672">=</span><span style="color:#ae81ff">2000</span> <span style="color:#ae81ff">\</span>
  --enable-v2<span style="color:#f92672">=</span>true
Restart<span style="color:#f92672">=</span>on-failure
RestartSec<span style="color:#f92672">=</span>5
LimitNOFILE<span style="color:#f92672">=</span>65536

<span style="color:#f92672">[</span>Install<span style="color:#f92672">]</span>
WantedBy<span style="color:#f92672">=</span>multi-user.target
</code></pre></div><p>加入<code> --enable-v2=true</code>，注意上一行结尾的反斜杠</p>
<ol>
<li>调整写入命令</li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">ETCDCTL_API<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> etcdctl   --endpoints<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>ETCD_ENDPOINTS<span style="color:#e6db74">}</span>   --ca-file<span style="color:#f92672">=</span>/opt/k8s/work/ca.pem   --cert-file<span style="color:#f92672">=</span>/opt/k8s/work/flanneld.pem   --key-file<span style="color:#f92672">=</span>/opt/k8s/work/flanneld-key.pem   mk <span style="color:#e6db74">${</span>FLANNEL_ETCD_PREFIX<span style="color:#e6db74">}</span>/config <span style="color:#e6db74">&#39;{&#34;Network&#34;:&#34;&#39;</span><span style="color:#e6db74">${</span>CLUSTER_CIDR<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;&#34;, &#34;SubnetLen&#34;: 21, &#34;Backend&#34;: {&#34;Type&#34;: &#34;vxlan&#34;}}&#39;</span>
</code></pre></div><p>FLANNEL_ETCD_PREFIX 环境变量需要自己手动加一下。我是随便给了个值。</p>
<h2 id="下载和分发-flanneld-二进制文件">下载和分发 flanneld 二进制文件</h2>
<p>从 flannel 的 <a href="https://github.com/coreos/flannel/releases">release 页面</a> 下载最新版本的安装包：</p>
<pre><code>cd /opt/k8s/work
mkdir flannel
wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz
tar -xzvf flannel-v0.11.0-linux-amd64.tar.gz -C flannel
</code></pre><p>分发二进制文件到集群所有节点：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp flannel/{flanneld,mk-docker-opts.sh} root@${node_ip}:/opt/k8s/bin/
    ssh root@${node_ip} &quot;chmod +x /opt/k8s/bin/*&quot;
  done
</code></pre><h2 id="创建-flannel-证书和私钥">创建 flannel 证书和私钥</h2>
<p>flanneld 从 etcd 集群存取网段分配信息，而 etcd 集群启用了双向 x509 证书认证，所以需要为 flanneld 生成证书和私钥。</p>
<p>创建证书签名请求：</p>
<pre><code>cd /opt/k8s/work
cat &gt; flanneld-csr.json &lt;&lt;EOF
{
  &quot;CN&quot;: &quot;flanneld&quot;,
  &quot;hosts&quot;: [],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [
    {
      &quot;C&quot;: &quot;CN&quot;,
      &quot;ST&quot;: &quot;BeiJing&quot;,
      &quot;L&quot;: &quot;BeiJing&quot;,
      &quot;O&quot;: &quot;k8s&quot;,
      &quot;OU&quot;: &quot;4Paradigm&quot;
    }
  ]
}
EOF
</code></pre><ul>
<li>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</li>
</ul>
<p>生成证书和私钥：</p>
<pre><code>cfssl gencert -ca=/opt/k8s/work/ca.pem \
  -ca-key=/opt/k8s/work/ca-key.pem \
  -config=/opt/k8s/work/ca-config.json \
  -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld
ls flanneld*pem
</code></pre><p>将生成的证书和私钥分发到<strong>所有节点</strong>（master 和 worker）：</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;mkdir -p /etc/flanneld/cert&quot;
    scp flanneld*.pem root@${node_ip}:/etc/flanneld/cert
  done
</code></pre><h2 id="向-etcd-写入集群-pod-网段信息">向 etcd 写入集群 Pod 网段信息</h2>
<p>注意：本步骤<strong>只需执行一次</strong>。</p>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
ETCDCTL_API=2 etcdctl \
  --endpoints=${ETCD_ENDPOINTS} \
  --ca-file=/opt/k8s/work/ca.pem \
  --cert-file=/opt/k8s/work/flanneld.pem \
  --key-file=/opt/k8s/work/flanneld-key.pem \
  mk ${FLANNEL_ETCD_PREFIX}/config '{&quot;Network&quot;:&quot;'${CLUSTER_CIDR}'&quot;, &quot;SubnetLen&quot;: 21, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;}}'
</code></pre><ul>
<li>flanneld <strong>当前版本 (v0.11.0) 不支持 etcd v3</strong>，故使用 etcd v2 API 写入配置 key 和网段数据；</li>
<li>写入的 Pod 网段 <code>${CLUSTER_CIDR}</code> 地址段（如 /16）必须小于 <code>SubnetLen</code>，必须与 <code>kube-controller-manager</code> 的 <code>--cluster-cidr</code> 参数值一致；</li>
</ul>
<h2 id="创建-flanneld-的-systemd-unit-文件">创建 flanneld 的 systemd unit 文件</h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
cat &gt; flanneld.service &lt;&lt; EOF
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
ExecStart=/opt/k8s/bin/flanneld \\
  -etcd-cafile=/etc/kubernetes/cert/ca.pem \\
  -etcd-certfile=/etc/flanneld/cert/flanneld.pem \\
  -etcd-keyfile=/etc/flanneld/cert/flanneld-key.pem \\
  -etcd-endpoints=${ETCD_ENDPOINTS} \\
  -etcd-prefix=${FLANNEL_ETCD_PREFIX} \\
  -iface=${IFACE} \\
  -ip-masq
ExecStartPost=/opt/k8s/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=always
RestartSec=5
StartLimitInterval=0

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service
EOF
</code></pre><ul>
<li><code>mk-docker-opts.sh</code> 脚本将分配给 flanneld 的 Pod 子网段信息写入 <code>/run/flannel/docker</code> 文件，后续 docker 启动时使用这个文件中的环境变量配置 docker0 网桥；</li>
<li>flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 <code>-iface</code> 参数指定通信接口;</li>
<li>flanneld 运行时需要 root 权限；</li>
<li><code>-ip-masq</code>: flanneld 为访问 Pod 网络外的流量设置 SNAT 规则，同时将传递给 Docker 的变量 <code>--ip-masq</code>（<code>/run/flannel/docker</code> 文件中）设置为 false，这样 Docker 将不再创建 SNAT 规则； Docker 的 <code>--ip-masq</code> 为 true 时，创建的 SNAT 规则比较“暴力”：将所有本节点 Pod 发起的、访问非 docker0 接口的请求做 SNAT，这样访问其他节点 Pod 的请求来源 IP 会被设置为 flannel.1 接口的 IP，导致目的 Pod 看不到真实的来源 Pod IP。 flanneld 创建的 SNAT 规则比较温和，只对访问非 Pod 网段的请求做 SNAT。</li>
</ul>
<h2 id="分发-flanneld-systemd-unit-文件到所有节点">分发 flanneld systemd unit 文件到<strong>所有节点</strong></h2>
<pre><code>cd /opt/k8s/work
source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    scp flanneld.service root@${node_ip}:/etc/systemd/system/
  done
</code></pre><h2 id="启动-flanneld-服务">启动 flanneld 服务</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld&quot;
  done
</code></pre><h2 id="检查启动结果-1">检查启动结果</h2>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh root@${node_ip} &quot;systemctl status flanneld|grep Active&quot;
  done
</code></pre><p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p>
<pre><code>journalctl -u flanneld
</code></pre><h2 id="检查分配给各-flanneld-的-pod-网段信息">检查分配给各 flanneld 的 Pod 网段信息</h2>
<p>查看集群 Pod 网段(/16)：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">source /opt/k8s/bin/environment.sh
ETCDCTL_API<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> etcdctl <span style="color:#ae81ff">\</span>
  --endpoints<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>ETCD_ENDPOINTS<span style="color:#e6db74">}</span> <span style="color:#ae81ff">\</span>
  --ca-file<span style="color:#f92672">=</span>/etc/kubernetes/cert/ca.pem <span style="color:#ae81ff">\</span>
  --cert-file<span style="color:#f92672">=</span>/etc/flanneld/cert/flanneld.pem <span style="color:#ae81ff">\</span>
  --key-file<span style="color:#f92672">=</span>/etc/flanneld/cert/flanneld-key.pem <span style="color:#ae81ff">\</span>
  get <span style="color:#e6db74">${</span>FLANNEL_ETCD_PREFIX<span style="color:#e6db74">}</span>/config
</code></pre></div><p>输出：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#f92672">{</span><span style="color:#e6db74">&#34;Network&#34;</span>:<span style="color:#e6db74">&#34;172.30.0.0/16&#34;</span>, <span style="color:#e6db74">&#34;SubnetLen&#34;</span>: 24, <span style="color:#e6db74">&#34;Backend&#34;</span>: <span style="color:#f92672">{</span><span style="color:#e6db74">&#34;Type&#34;</span>: <span style="color:#e6db74">&#34;vxlan&#34;</span><span style="color:#f92672">}</span><span style="color:#f92672">}</span>
</code></pre></div><p>查看已分配的 Pod 子网段列表(/24):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">source /opt/k8s/bin/environment.sh
ETCDCTL_API<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> etcdctl <span style="color:#ae81ff">\</span>
  --endpoints<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>ETCD_ENDPOINTS<span style="color:#e6db74">}</span> <span style="color:#ae81ff">\</span>
  --ca-file<span style="color:#f92672">=</span>/etc/kubernetes/cert/ca.pem <span style="color:#ae81ff">\</span>
  --cert-file<span style="color:#f92672">=</span>/etc/flanneld/cert/flanneld.pem <span style="color:#ae81ff">\</span>
  --key-file<span style="color:#f92672">=</span>/etc/flanneld/cert/flanneld-key.pem <span style="color:#ae81ff">\</span>
  ls <span style="color:#e6db74">${</span>FLANNEL_ETCD_PREFIX<span style="color:#e6db74">}</span>/subnets
</code></pre></div><p>输出（结果视部署情况而定）：</p>
<pre><code>/kubernetes/network/subnets/172.30.80.0-24
/kubernetes/network/subnets/172.30.32.0-24
/kubernetes/network/subnets/172.30.184.0-24
</code></pre><p>查看某一 Pod 网段对应的节点 IP 和 flannel 接口地址:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">source /opt/k8s/bin/environment.sh
ETCDCTL_API<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> etcdctl <span style="color:#ae81ff">\</span>
  --endpoints<span style="color:#f92672">=</span><span style="color:#e6db74">${</span>ETCD_ENDPOINTS<span style="color:#e6db74">}</span> <span style="color:#ae81ff">\</span>
  --ca-file<span style="color:#f92672">=</span>/etc/kubernetes/cert/ca.pem <span style="color:#ae81ff">\</span>
  --cert-file<span style="color:#f92672">=</span>/etc/flanneld/cert/flanneld.pem <span style="color:#ae81ff">\</span>
  --key-file<span style="color:#f92672">=</span>/etc/flanneld/cert/flanneld-key.pem <span style="color:#ae81ff">\</span>
  get <span style="color:#e6db74">${</span>FLANNEL_ETCD_PREFIX<span style="color:#e6db74">}</span>/subnets/172.30.80.0-24
</code></pre></div><p>输出（结果视部署情况而定）：</p>
<pre><code>{&quot;PublicIP&quot;:&quot;172.27.137.240&quot;,&quot;BackendType&quot;:&quot;vxlan&quot;,&quot;BackendData&quot;:{&quot;VtepMAC&quot;:&quot;ce:9c:a9:08:50:03&quot;}}
</code></pre><ul>
<li>172.30.80.0/21 被分配给节点 zhangjun-k8s01（172.27.137.240）；</li>
<li>VtepMAC 为 zhangjun-k8s01 节点的 flannel.1 网卡 MAC 地址；</li>
</ul>
<h2 id="检查节点-flannel-网络信息">检查节点 flannel 网络信息</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell"><span style="color:#f92672">[</span>root@zhangjun-k8s01 work<span style="color:#f92672">]</span><span style="color:#75715e"># ip addr show</span>
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">65536</span> qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens192: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1500</span> qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:b1:9e:42 brd ff:ff:ff:ff:ff:ff
    inet 10.20.250.23/24 brd 10.20.250.255 scope global noprefixroute ens192
       valid_lft forever preferred_lft forever
    inet6 fe80::3bb2:f3d:e0c7:b4c6/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
3: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu <span style="color:#ae81ff">1500</span> qdisc noqueue state DOWN group default qlen 1000
    link/ether 52:54:00:76:e9:a9 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
       valid_lft forever preferred_lft forever
4: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu <span style="color:#ae81ff">1500</span> qdisc pfifo_fast master virbr0 state DOWN group default qlen 1000
    link/ether 52:54:00:76:e9:a9 brd ff:ff:ff:ff:ff:ff
6: dummy0: &lt;BROADCAST,NOARP&gt; mtu <span style="color:#ae81ff">1500</span> qdisc noop state DOWN group default qlen 1000
    link/ether 6e:35:ce:47:f1:bf brd ff:ff:ff:ff:ff:ff
7: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu <span style="color:#ae81ff">1500</span> qdisc noop state DOWN group default 
    link/ether ae:21:d2:74:e2:1b brd ff:ff:ff:ff:ff:ff
    inet 10.254.0.1/32 brd 10.254.0.1 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
8: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1450</span> qdisc noqueue state UNKNOWN group default 
    link/ether 36:8d:2b:f5:04:c9 brd ff:ff:ff:ff:ff:ff
    inet 172.30.232.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::348d:2bff:fef5:4c9/64 scope link 
       valid_lft forever preferred_lft forever
</code></pre></div><ul>
<li>flannel.1 网卡的地址为分配的 Pod 子网段的第一个 IP（.0），且是 /32 的地址；</li>
</ul>
<pre><code>[root@zhangjun-k8s01 work]# ip route show |grep flannel.1
172.30.32.0/24 via 172.30.32.0 dev flannel.1 onlink
172.30.184.0/24 via 172.30.184.0 dev flannel.1 onlink
</code></pre><ul>
<li>到其它节点 Pod 网段请求都被转发到 flannel.1 网卡；</li>
<li>flanneld 根据 etcd 中子网段的信息，如 <code>${FLANNEL_ETCD_PREFIX}/subnets/172.30.80.0-24</code> ，来决定进请求发送给哪个节点的互联 IP；</li>
</ul>
<h2 id="验证各节点能通过-pod-网段互通">验证各节点能通过 Pod 网段互通</h2>
<p>在<strong>各节点上部署</strong> flannel 后，检查是否创建了 flannel 接口(名称可能为 flannel0、flannel.0、flannel.1 等)：</p>
<pre><code>source /opt/k8s/bin/environment.sh
for node_ip in ${NODE_IPS[@]}
  do
    echo &quot;&gt;&gt;&gt; ${node_ip}&quot;
    ssh ${node_ip} &quot;/usr/sbin/ip addr show flannel.1|grep -w inet&quot;
  done
</code></pre><p>输出：</p>
<pre><code>&gt;&gt;&gt; 172.27.137.240
    inet 172.30.80.0/32 scope global flannel.1
&gt;&gt;&gt; 172.27.137.239
    inet 172.30.32.0/32 scope global flannel.1
&gt;&gt;&gt; 172.27.137.238
    inet 172.30.184.0/32 scope global flannel.1
</code></pre><p>在各节点上 ping 所有 flannel 接口 IP，确保能通：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">source /opt/k8s/bin/environment.sh
<span style="color:#66d9ef">for</span> node_ip in <span style="color:#e6db74">${</span>NODE_IPS[@]<span style="color:#e6db74">}</span>
  <span style="color:#66d9ef">do</span>
    echo <span style="color:#e6db74">&#34;</span><span style="color:#e6db74">&gt;&gt;&gt; </span><span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
    ssh <span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;ping -c 1 172.30.80.0&#34;</span>
    ssh <span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;ping -c 1 172.30.32.0&#34;</span>
    ssh <span style="color:#e6db74">${</span>node_ip<span style="color:#e6db74">}</span> <span style="color:#e6db74">&#34;ping -c 1 172.30.184.0&#34;</span>
  <span style="color:#66d9ef">done</span>
</code></pre></div><h1 id="验证集群功能">验证集群功能</h1>
<h2 id="创建测试文件">创建测试文件</h2>
<pre><code>cd /opt/k8s/work
cat &gt; nginx-ds.yml &lt;&lt;EOF
apiVersion: v1
kind: Service
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
spec:
  type: NodePort
  selector:
    app: nginx-ds
  ports:
  - name: http
    port: 80
    targetPort: 80
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  selector:
    matchLabels:
      app: nginx-ds
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
EOF
</code></pre><h2 id="执行测试">执行测试</h2>
<pre><code>kubectl create -f nginx-ds.yml
</code></pre>
    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>作者:</span>
                <span>linjinbao66 </span>
                </p>
            
    </div>

    <div class="post-tags">
        
        <section>
                <a href="javascript:window.history.back();">返回</a></span> · 
                <span><a href="https://linjinbao.github.io">主页</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://linjinbao.github.io/2020/2020-10-16-k8s%E7%9A%84rbac%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/" class="prev" rel="prev" title="k8s的rbac权限控制"><i class="iconfont icon-left"></i>&nbsp;k8s的rbac权限控制</a>
         
        
        <a href="https://linjinbao.github.io/2020/about/" class="next" rel="next" title="关于">关于&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
		  <div id="utter-container"></div>
    <script src="https://utteranc.es/client.js"
        repo= 'linjinbao/linjinbao.github.io'
        issue-term= 'title'
        label= 'utterance'
        theme= 'github-dark-orange'
        crossorigin="anonymous"
        async>
    </script>

    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2019 - 2021</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i> 
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="https://linjinbao.github.io">linjinbao66</a>  </span> 
         

         
		 
    </div>
</footer>












    
    
    <script src="/js/vendor_no_gallery.min.js" async=""></script>
    
  



     </div>
  </body>
</html>
