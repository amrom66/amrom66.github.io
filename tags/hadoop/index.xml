<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hadoop on 打工人日记</title>
    <link>https://linjinbao.github.io/tags/hadoop/</link>
    <description>Recent content in hadoop on 打工人日记</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://linjinbao.github.io/tags/hadoop/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>hadoop学习</title>
      <link>https://linjinbao.github.io/2020/2020-02-12-hadoop/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://linjinbao.github.io/2020/2020-02-12-hadoop/</guid>
      <description>hadoop学习 spark 定义：基于内存计算的大数据并行计算框架，可用于构建大型的、低延迟的数据分析应用程序。
spark-shell使用 scala&amp;gt; val textFile = spark.read.textFile(&amp;#34;hdfs://c1:9000/redis.conf&amp;#34;) textFile: org.apache.spark.sql.Dataset[String] = [value: string] scala&amp;gt; textFile.count() res4: Long = 1372 scala&amp;gt; val wordsRdd=textFile.flatMap(line=&amp;gt;line.split(&amp;#34; &amp;#34;)) ##统计字数 </description>
    </item>
    
    <item>
      <title>hadoop全分布式集群搭建</title>
      <link>https://linjinbao.github.io/2020/2020-01-27-hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://linjinbao.github.io/2020/2020-01-27-hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</guid>
      <description>hadoop全分布式集群搭建 准备虚拟机 192.168.126.133 cent1 namenode datanode 192.168.126.130 cent2 namenode datanode secondary 192.168.126.139 vm1 datanode 192.168.126.140 vm2 datanode
export JAVA_HOME=/usr export HADOOP_HOME=/root/hadoop-3.1.3 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin/
yum install ntpdate ntpdate s2c.time.edu.cn timedatectl set-timezone Asia/Shanghai
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:/root/zookeeper/bin</description>
    </item>
    
    <item>
      <title>hive on spark 测试</title>
      <link>https://linjinbao.github.io/2020/2020-01-09-hive/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://linjinbao.github.io/2020/2020-01-09-hive/</guid>
      <description>hive on spark 测试 准备工作： 建库：
create database db2; 建表：
sql语句：
create table tb1(id int , name string); 测试1：插入单条数据 insert into tb1(id, name) values(10, &amp;#39;linjb&amp;#39;); 执行结果：1秒
Status: Running (Hive on Spark job[1]) Job Progress Format CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount [StageCost] -------------------------------------------------------------------------------------- STAGES ATTEMPT STATUS TOTAL COMPLETED RUNNING PENDING FAILED -------------------------------------------------------------------------------------- Stage-1 ........ 0 FINISHED 1 1 0 0 0 -------------------------------------------------------------------------------------- STAGES: 01/01 [==========================&amp;gt;&amp;gt;] 100% ELAPSED TIME: 1.00 s -------------------------------------------------------------------------------------- Status: Finished successfully in 1.</description>
    </item>
    
    <item>
      <title>spark on hive 踩坑</title>
      <link>https://linjinbao.github.io/2020/2020-01-08-spark/</link>
      <pubDate>Wed, 08 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://linjinbao.github.io/2020/2020-01-08-spark/</guid>
      <description>spark on hive 踩坑 搭建hadoop环境 掠过
搭建hive环境 掠过
搭建spark  注意版本兼容  在pom文件中查看
 下载spark-2.0.0-bin-hadoop2.4-without-hive版本，without-hive必须  我的版本 hive:2.3.6 spark :spark-2.0.0-bin-hadoop2.4-without-hive hadoop:2.7
  复制jar包
 cp scala-library-*.jar /hive_home/lib/ cp spark-core_*.jar /hive_home/lib/ cp spark-network-common_*.jar /hive_home/lib/ chill-java chill jackson-module-paranamer jackson-module-scala jersey-container-servlet-core jersey-server json4s-ast kryo-shaded minlog scala-xml spark-launcher spark-network-shuffle spark-unsafe xbean-asm5-shaded  从spark的jars文件夹中复制过去
  配置hive-site.xml
&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.enable.spark.execution.engine&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;spark.master&amp;lt;/name&amp;gt; &amp;lt;--! &amp;lt;value&amp;gt;spark://localhost:7077&amp;lt;/value&amp;gt; --&amp;gt; &amp;lt;value&amp;gt;local&amp;lt;/value&amp;gt; &amp;lt;description/&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.execution.engine&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;spark&amp;lt;/value&amp;gt; &amp;lt;description&amp;gt; Expects one of [mr, tez, spark].</description>
    </item>
    
    <item>
      <title>hive踩坑纪实</title>
      <link>https://linjinbao.github.io/2019/2019-12-29-hive%E8%B8%A9%E5%9D%91/</link>
      <pubDate>Sun, 29 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://linjinbao.github.io/2019/2019-12-29-hive%E8%B8%A9%E5%9D%91/</guid>
      <description>hive踩坑纪实 hive查询报错 0: jdbc:hive2://localhost:10000&amp;gt; select * from testa; Error: Error while compiling statement: FAILED: SemanticException Unable to determine if hdfs://localhost:9000/user/hive/warehouse/testa is encrypted: java.lang.IllegalArgumentException: Wrong FS: hdfs://localhost:9000/user/hive/warehouse/testa, expected: hdfs://hadoop-master:9000 (state=42000,code=40000) 0: jdbc:hive2://localhost:10000&amp;gt; 原因：
由于之前配置的hdfs-site.xml中：
&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hdfs://localhost:9000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; 修改成：
&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hdfs://hadoop-master:9000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; 解决办法：
修改元数据库：
use hive; update DBS set DB_LOCATION_URI=REPLACE (DB_LOCATION_URI,&amp;#39;localhost&amp;#39;,&amp;#39;hadoop-master&amp;#39;) update SDS set LOCATION=REPLACE (LOCATION,&amp;#39;localhost&amp;#39;,&amp;#39;hadoop-master&amp;#39;); hive用户权限管理 第一步：配置hive-site.xml
&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.users.in.admin.role&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;root&amp;lt;/value&amp;gt; &amp;lt;description&amp;gt;定义超级管理员 启动的时候会自动创建Comma separated list of users who are in admin role for bootstrapping.</description>
    </item>
    
    <item>
      <title>hadoop-hdfs学习</title>
      <link>https://linjinbao.github.io/2019/2019-12-26-hdfs%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Thu, 26 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://linjinbao.github.io/2019/2019-12-26-hdfs%E5%AD%A6%E4%B9%A0/</guid>
      <description>hadoop-hdfs学习 概念 hdfs是一个分布式的，文件存储系统 ，重要特性如下：
 分块存储 统一的抽象目录树 不支持文件修改  shell操作 hadoop fs -ls /：列出文件
hadoop fs -ls hdfs://hadoop-server01:9000/：列出文件
hadoop fs -mkdir -p /aaa/bbb/cc/dd：创建目录
hadoop fs -moveFromLocal D:\public /aaa/bbb：本地文件移动到hdfs
hadoop fs -moveToLocal /aaa/bbb/ d:\fff：hdfs文件移动到本地
Hadoop fs -appendToFile ./hello.txt /hello.txt：文件追加
hadoop fs -cat /hello.txt：显示文件按内容
hadoop fs -tail /weblog/access_log.1：显示文件末尾
hadoop fs -text /weblog/access_log.1：以字符形式打印文件内容
hadoop fs -chmod 755 /aaa/bbb/public：修改文件权限，和linux一致
hadoop fs -copyFromLocal ./jdk.tar.gz /aaa/：拷贝
hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2：在hdfs上拷贝
hadoop fs -mv /aaa/jdk.tar.gz /：在hdfs上移动</description>
    </item>
    
    <item>
      <title>hive学习01-环境搭建</title>
      <link>https://linjinbao.github.io/2019/2019-12-25-hive%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Wed, 25 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://linjinbao.github.io/2019/2019-12-25-hive%E5%AD%A6%E4%B9%A0/</guid>
      <description>hive学习01-环境搭建 环境准备：
 jdk 自己手动放置在一个不含空格的目录，并配置好JAVA_HOME hadoop安装包 hadoop-2.7.7 http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz hive 安装包hive-2.1.1 http://archive.apache.org/dist/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz  环境准备玩本人目录如下：
&amp;ndash;D:\hadoop
​ &amp;ndash;hadoop-2.7.7
​ &amp;ndash;jdk1.8.0_171
​ &amp;ndash;apache-hive-2.1.1-bin
hadoop环境搭建 第零步：配置环境变量：HADOOP_HOME=D:\hadoop\hadoop-2.7.7
第一步：修改配置文件：
 core-site.xml  &amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hdfs://localhost:9000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/configuration&amp;gt;  hdfs-site.xml  &amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.name.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;file:/hadoop/data/dfs/namenode&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.datanode.data.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;file:/hadoop/data/dfs/datanode&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/configuration&amp;gt;  mapred-site.xml（mapred-site.xml.template修改而来）  &amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/configuration&amp;gt;  yarn-site.xml  &amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;org.apache.hadoop.mapred.ShuffleHandler&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/configuration&amp;gt;  hadoop-env.</description>
    </item>
    
  </channel>
</rss>